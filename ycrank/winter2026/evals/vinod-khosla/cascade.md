# Cascade -- Vinod Khosla Evaluation

The first thing I notice about Cascade is that nobody is laughing at them. When I backed Juniper to challenge Cisco, Cisco's CTO told me they'd never build a TCP/IP router. When I invested in Impossible Foods, people looked at me like I'd lost my mind. When I wrote my largest check ever to OpenAI in 2019, most of my peers thought AI was a research curiosity. The signal I look for is experts dismissing the idea — because that's where the outsiders haven't crowded in and the consequence of success is enormous. Cascade's problem — "AI agents need safety infrastructure" — is the opposite. Gartner is writing reports about it. F5 just paid $180 million for CalypsoAI. WitnessAI has $58 million. Fiddler AI has $100 million. Meta open-sourced LlamaFirewall. This is not a space where the establishment is dismissive. This is a space where the establishment has already arrived, set up camp, and is arguing about which shade of blue the logo should be.

Let me apply my consequence-magnitude test. If Cascade succeeds completely — dominates their category — what changes? Enterprises deploying AI agents have better monitoring and safety tools. That's a fine business. Maybe a $5 billion outcome at the extreme end. But nobody's life changes in a fundamental way. No trillion-dollar industry gets displaced. The world doesn't operate differently. Compare that to the consequence magnitude I saw with OpenAI — the arrival of artificial general intelligence — or with Impossible Foods — the elimination of animal agriculture. Cascade is building crash helmets for other people's motorcycles. The crash helmet market is real, but it follows the motorcycle market; it never leads it. And when the motorcycle manufacturers start integrating helmets into the frame — which is exactly what OpenAI, Anthropic, and Google are doing with built-in safety features — the standalone helmet company has a structural problem. The consequence of success here is bounded by the fact that they're a supporting layer, not the primary innovation.

The bull case deserves genuine engagement. Autonomous AI agents may represent a new computing paradigm as significant as the shift from client-server to cloud. If Gartner's prediction holds — 40% of enterprise applications featuring AI agents by late 2026 — then the safety infrastructure layer is potentially analogous to what Cloudflare became for web applications or what CrowdStrike became for endpoints. The "self-improving models" concept, if it functions as a genuine closed-loop system that compounds learning from production failures across customer environments, could create a data moat that static guardrails companies cannot replicate. And there's a structural tension worth noting: AI platform providers face a conflict in simultaneously selling agent capabilities and independently auditing their safety, which could preserve space for an independent guardian layer. If all of that materializes, this could be a meaningful company. But three things would need to be simultaneously true: the self-improving technology actually works and is architecturally differentiated (no evidence yet), enterprises choose an independent layer over built-in platform safety (uncertain), and Cascade outcompetes five better-funded competitors who started earlier (difficult). That's too many simultaneous low-probability requirements — the founder hasn't prioritized which risk to eliminate first.

The founders are competent but conventional. Both from UC Berkeley, both from BAIR, one with AI safety research under Dawn Song, the other with production monitoring at Netflix and Amazon. These are exactly the credentials I'd expect from founders building an AI safety platform — which is precisely the problem. They're insiders applying their academic training to a commercially obvious category. Where is the "irreverence, foolish confidence and naivety" that I believe creates the greatest technology companies? Where is the outsider perspective that frees them to attempt something the industry considers impossible? Building AI guardrails after working in an AI safety lab is the expected career path, not the contrarian one. I see no evidence in the dossier of rapid learning-rate evolution, pivoted assumptions, or abandoned hypotheses. They appear to be executing a plan that any two smart Berkeley graduates could have designed, in a space that any informed observer could identify as promising.

The technology differentiation is the weakest element. "Self-improving safety and reliability models" is marketing language, not demonstrated architecture. Every AI-powered security company since Darktrace has claimed to learn from threats in production. The question is whether Cascade's approach is structurally different from what Guardrails AI, WitnessAI, Fiddler AI, and platform-native safety features already offer — and the dossier provides zero evidence it is. Meanwhile, the commoditization risk is severe: Meta's open-sourcing of LlamaFirewall, Invariant Labs' open-source agent guardrails framework, and the platform providers' integration of safety features into their SDKs mean the baseline capability is free. Cascade would need to demonstrate that their self-improving models produce meaningfully better outcomes than what's available at zero cost, and then they'd need to demonstrate it against competitors with 10-20x more capital. That's not an impossible climb, but it's the kind of challenge that requires a genuinely differentiated technology — not a feature claim on a website that returned only CSS when researchers tried to access it.

I'll be direct: this company has narrowed its ambition to a space where success is probable enough that the consequence is bounded. They've picked a real market, assembled the right credentials, and positioned themselves in a trending category. That's exactly the playbook I warn against. They've reduced risk to the point where the upside is a decent enterprise security company in a crowded market. I'd rather back two founders attacking a problem that terrifies them and matters at civilization scale — even at 90% failure probability — than two founders optimizing for a manageable niche in AI safety infrastructure.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Consequence Magnitude If Successful | 9/30 |
| Founder Learning Rate and Contrarian Courage | 10/25 |
| Technology Disruption Potential vs. Incumbent Systems | 7/20 |
| Rate of Change and Timing Trajectory | 8/15 |
| Gene Pool Engineering and Team Construction | 5/10 |
| **Total** | **39/100** |

**Total Score: 39/100** (Pass)
