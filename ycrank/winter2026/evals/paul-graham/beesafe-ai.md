# BeeSafe AI -- Paul Graham Evaluation

The first thing I notice about BeeSafe is the schlep. These founders are building AI that spends weeks talking to criminals. Not in a sanitized, academic sense — their Chatterbox system ran for seven weeks and sustained conversations with 568 scammers, averaging 44 messages and nearly 8 days per interaction. Think about what that means operationally. You're building software that has to be convincing enough to fool professional con artists for days on end, while extracting mule account numbers and crypto wallet addresses from people who are actively trying to steal money. Then you have to package that intelligence for banks and government agencies with their compliance requirements. This is exactly the kind of multi-layered ugliness that makes smart engineers decide they'd rather build another AI copilot for something. The legal gray areas alone — deploying deceptive AI agents against criminal actors — would cause most founders to unconsciously look the other way.

The origin story here passes my organic test cleanly. Ariana Mirian spent a decade in security measurement research under Stefan Savage and Geoffrey Voelker at UCSD — these are two of the most prominent internet security researchers alive. Daniel Spokoyny was a postdoc in the same group, co-leading the specific initiative on social engineering scams. Nikolai Vogler is a PhD student there. They built the system as research, deployed it, published the paper ("Victim as a Service," October 2025), and then started the company. The paper came first. The startup came second. This is the opposite of sitting down to brainstorm startup ideas. These founders have been living inside the scam ecosystem through years of academic research and stumbled into a commercial application of work they were already doing.

What I find particularly interesting is how the schlep creates structural moat here, similar to how Stripe's willingness to deal with banks and card networks kept competitors away for years. Publishing the approach in an academic paper might seem like giving away the secret, but knowing how to engage scammers at scale is like knowing that processing payments is a good business — the knowledge doesn't make anyone more willing to do the work. You still need to build agents that can sustain adversarial conversations across platforms, handle the legal exposure of interacting with criminal operations, manage an ongoing arms race as scammers adapt, and then sell the resulting intelligence to compliance-heavy enterprise buyers. Apate AI in Australia is the closest competitor, but they're focused on telecom voice calls with Australian banks. The market isn't winner-take-all, and the schlep is large enough that two entrants don't crowd it.

The technical founder signal is strong. Three PhDs, all from top programs (CMU LTI, UCSD CSE), all co-authors on the foundational paper, and they've already built and deployed the working system themselves. Daniel has 37 GitHub repos and research stints at Google AI, Microsoft Research, and Salesforce. This is a team that writes code, not a team that plans to hire engineers after raising. The Chatterbox deployment — 4,725 scammer contacts in seven weeks — is proof they can ship something real, not just publish papers about it.

Where I hesitate is on user pull and the transition from research to revenue. The YC page claims their intelligence has "enabled financial services and government agencies to intercept scammers mid-transaction," but there are no named customers, no revenue figures, no LOIs. That language could mean a paid contract or it could mean they shared findings with a government contact who said "this is useful." The NSF SBIR ($305K) and the NSIN Cyber Innovators win validate the technical approach within government circles, but government validation and government purchasing are very different things. A three-person academic team selling into financial institutions and federal agencies — entities with 6-18 month procurement cycles — is a genuine execution risk. The strongest bull case would require them to close one or two lighthouse customers within the next two quarters, proving they can convert research credibility into enterprise contracts. If they can do that, the compounding data asset from each scammer engagement makes this increasingly hard to replicate.

The problem is real — $12 billion in annual US losses from trust-based scams, with existing fraud detection built for the wrong threat model (unauthorized transactions, not socially engineered authorized ones). The timing is right — LLMs just crossed the capability threshold where sustained multi-day deceptive conversations became feasible, and regulatory attention on pig butchering intensified through 2025. The founders found this problem organically and are willing to do work that repels most people. What I can't yet see is evidence that anyone is paying for it. At pre-seed from YC, that absence isn't disqualifying, but it means I'm betting on the founders and the schlep rather than demonstrated demand. Given the organic origin, the technical depth, and the structural deterrence the schlep provides, I'd make a small bet here — but I'd want to see a signed customer within months, not quarters.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 24/30 |
| Relentlessly Resourceful Founders | 15/25 |
| Evidence of Wanting: Demonstrated User Pull | 8/20 |
| Technical Hacker Founders Who Build | 13/15 |
| Growth Trajectory and Default Alive Economics | 5/10 |
| **Total** | **65/100** |

**Total Score: 65/100** (Invest)
