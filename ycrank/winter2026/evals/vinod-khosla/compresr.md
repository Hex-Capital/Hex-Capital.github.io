# Compresr -- Vinod Khosla Evaluation

Here's the fundamental problem with Compresr: they're building a toll booth on a highway where the speed limit keeps going up and the toll keeps going down. LLM token prices are deflating at roughly 10x per year. Anthropic already ships prompt caching. OpenAI offers cached context discounts. Every six months, the major providers expand context windows and cut per-token prices. Compresr's entire value proposition -- saving money on tokens -- is locked in a race against the most powerful companies on Earth systematically eliminating the very pain point Compresr monetizes. The dossier itself identifies this risk plainly: "If token costs become negligible, the economic motivation to compress context diminishes." That's not a risk factor. That's a description of the trajectory we're already on.

Let me be direct about the consequence of success. If Compresr achieves everything it's attempting -- becomes the universal compression layer for every LLM API call -- what changes? Companies pay less for inference. Agents run a bit faster. Nobody's life is fundamentally different. No industry is displaced. No incumbent system becomes obsolete. This is cost optimization middleware. I've seen hundreds of companies like this -- smart teams building useful infrastructure that makes something 30% cheaper. The team is strong, the market exists, the technology works. And that's where I stop. These founders have done something I see too often: they've picked a problem tractable enough to solve and a market real enough to validate, and in doing so they've guaranteed that even maximum success produces a modest outcome. They're optimizing their way into inconsequence.

The bull case deserves honest engagement. Agentic AI workflows are about to create an explosion in token consumption -- every agent session could burn millions of tokens across conversation history, tool traces, and retrieved context. Even with price deflation, absolute LLM spend is doubling annually (to $8.4B per Menlo Ventures). If compression becomes critical infrastructure for agents the way CDNs became critical for web traffic, Compresr could occupy a genuinely valuable position. And compression isn't just cost -- it's accuracy. Overstuffed context windows degrade model performance, so intelligent compression could be a quality play that survives price deflation. The team's EPFL DLab research in prompt compression gives them real technical depth at the exact intersection that matters. If I squint, I can see a version of this where the agentic explosion creates so much token demand that compression becomes non-optional infrastructure. But here's why that analogy breaks: CDN providers succeeded because content creators couldn't make their files smaller and ISPs couldn't make bandwidth free. LLM providers *can* make their models more efficient, *can* expand context windows, and *can* cut prices -- and they're doing all three simultaneously. Compresr is building critical middleware on a platform whose owners have both the technical ability and economic incentive to commoditize the compression layer.

The team composition reinforces my concern. Four EPFL students from the same institution, all technical, all from research backgrounds. Ivan Zakazov researched LLM context compression at EPFL; Oussama Gabouj worked on prompt compression at EPFL's DLab. They are domain experts executing their research as a product. That's the opposite of the pattern that excites me. My best investments -- Juniper, Impossible Foods, Rocket Lab -- came from outsiders who didn't know what the established field considered impossible. These founders know exactly what the field considers possible, because they are the field. They're building something the consensus already validates: Microsoft Research published LLMLingua (5.8k GitHub stars), there are papers at EMNLP, ACL, and NAACL on prompt compression, and the approach is well-understood. When incumbents and academics validate the technology rather than dismissing it, the innovation is usually incremental enough to be non-threatening. No one is telling these founders they're crazy -- and that's the problem.

The competitive dynamics are unfavorable in a way that's hard to overcome with execution alone. LLMLingua is free, open-source, and already integrated into LangChain and LlamaIndex. Compresr's differentiation is "managed API versus self-hosted library" -- a packaging advantage, not a technology moat. Meanwhile, Factory.ai has embedded compression into their coding agent at $300M valuation, and Inferact is optimizing at the inference engine level with $150M from a16z. The compression layer is being attacked from below (LLM providers reducing prices), from above (agent platforms building it in), and from the side (open-source alternatives). No published benchmarks demonstrate Compresr's claimed superiority over these alternatives. The 102 GitHub stars on Context-Gateway suggest early developer curiosity, not production adoption at scale. I'd need to see evidence that their compression quality is not just marginally but dramatically better than the free alternatives -- and that evidence doesn't exist in any public form.

These are smart people working on a real problem. But the problem isn't big enough to justify the career risk, and the structural dynamics of the market are working against them. I'd tell this team: you understand LLMs deeply, you understand compression deeply, you have genuine technical talent. Stop building a toll booth on someone else's highway. Go find a problem where your understanding of efficient AI systems could change how an entire industry operates -- not how much it pays for API calls.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Consequence Magnitude If Successful | 8/30 |
| Founder Learning Rate and Contrarian Courage | 9/25 |
| Technology Disruption Potential vs. Incumbent Systems | 7/20 |
| Rate of Change and Timing Trajectory | 7/15 |
| Gene Pool Engineering and Team Construction | 4/10 |
| **Total** | **35/100** |

**Total Score: 35/100** (Pass)
