# RunAnywhere -- Vinod Khosla Evaluation

RunAnywhere is building in a space where the three most powerful technology companies on Earth -- Meta, Google, and Apple -- are already giving away open-source inference runtimes for free. ExecuTorch hit 1.0 GA and powers on-device AI for billions of Meta users. MediaPipe covers Google's ecosystem. Apple's MLX owns Apple Silicon. These are not hypothetical competitors -- they are shipping products at a scale RunAnywhere cannot match, and their incentive is to make on-device inference a commodity because it drives adoption of their hardware and platforms. Against this backdrop, RunAnywhere's thesis is that it can build a commercial control plane -- essentially a management dashboard for model deployment -- on top of inference engines these giants are distributing at zero cost. That is not a battle I want to fund. When Cisco told me they would never do a TCP/IP router, that meant the incumbent couldn't see the disruption coming. Here, the incumbents are literally building the disruption and handing it out.

The consequence-of-success question answers itself quickly. If RunAnywhere succeeds completely -- captures the on-device AI deployment market, becomes the default SDK, and monetizes its control plane -- what changes? Developers deploy models to phones with fewer lines of code. Enterprise teams get a dashboard for OTA updates and A/B testing. The edge AI software market reaches maybe $9 billion by 2030. This is a useful business in a real market. It is not a business that restructures an industry or makes the world operate differently. "Ollama but for mobile, with a cloud fallback" is a product description that tells me exactly how bounded the ambition is. Nobody says "Ollama but for mobile" when they're trying to change civilization. They say it when they're trying to build a convenient developer tool. I have nothing against convenient developer tools, but I don't invest my own money in them. The consequence of full success here is a mid-tier infrastructure company, not an outcome that justifies tolerating a 90% failure probability.

The technology itself reinforces this concern. The dossier states plainly that "the individual components -- local model inference, model downloading, OTA updates -- are technically reproducible." llama.cpp handles CPU inference efficiently. ExecuTorch provides a production runtime. The control plane layer -- policy routing, fleet analytics, OTA delivery -- is standard distributed systems work that any well-funded DevTools company could build. There is no proprietary algorithm, no novel capability, no technology breakthrough that makes the incumbent approach uneconomic. RunAnywhere is an integration layer. A well-executed one, perhaps -- 9,000 GitHub stars and 1,663 commits from a two-person team demonstrate real engineering output. But integration layers get commoditized. The cross-platform breadth (iOS, Android, React Native, Flutter, web) creates surface area, not a moat. Nexa AI already has $3.4 million in revenue, 31 people, and a two-year head start in the same space. That is not a competitive gap that a control plane dashboard closes.

The strongest bull case is timing. On-device AI is genuinely at an inflection point -- small models crossed quality thresholds in 2024-2025, NPU hardware is shipping in consumer devices, privacy regulations are creating real demand for local processing. If I squint, I can see a world where on-device AI becomes as ubiquitous as cloud AI, and the infrastructure layer that manages deployment at scale becomes extremely valuable. The Docker analogy is relevant: containerization was obvious in retrospect, and Docker captured the developer mindset at the right moment. If RunAnywhere captures the on-device deployment mindset before the incumbents build their own fleet management tools, there is a path to a meaningful company. But Docker is worth roughly $5 billion -- a strong outcome for a VC fund, but not the kind of 100x consequence-of-success that makes me ignore the probability of failure. And the timing, while good, is consensus -- every major platform company is investing in on-device AI right now. This is not 2019 AI or 2004 cleantech, where I was early and alone. This is a crowded party where Meta and Google already control the music.

The founders are two RIT software engineers -- one from Intuit, one from AWS -- building exactly what their resumes would predict. There is no outsider perspective here, no contrarian thesis that the industry dismisses. They are not biochemists attempting to replace meat or New Zealanders building rockets. They are infrastructure engineers building infrastructure, which is the most predictable possible founding story. I see no evidence of the kind of irreverence, foolish confidence, or willingness to attempt the impossible that characterized the founders of my best investments. The team is homogeneous -- same university, same discipline, no one addressing the enterprise sales risk that is critical to their monetization thesis. A two-person engineering team positioning for enterprise fleet management is a mismatch I have seen fail repeatedly.

I pass. These are competent builders in a real market with reasonable timing, and I suspect they will build something useful. But the magnitude of the outcome, even in the best case, does not clear my threshold. The world does not need me to fund a slightly better model deployment SDK. It needs me to fund things that terrify the incumbents, and Meta, Google, and Apple are not losing sleep over RunAnywhere's control plane.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Consequence Magnitude If Successful | 8/30 |
| Founder Learning Rate and Contrarian Courage | 6/25 |
| Technology Disruption Potential vs. Incumbent Systems | 6/20 |
| Rate of Change and Timing Trajectory | 10/15 |
| Gene Pool Engineering and Team Construction | 4/10 |
| **Total** | **34/100** |

**Total Score: 34/100** (Pass)
