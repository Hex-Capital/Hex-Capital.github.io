# Haladir -- Cyan Banister Evaluation

The first thing I'd want to know about Haladir is the thing the dossier can't tell me: why do four undergraduates -- a CMU freshman who runs cross country, two UVA students, and a Princeton CS major -- wake up one morning and decide they're going to sell reinforcement learning training infrastructure to Anthropic and OpenAI? That's my opening question with every founder, always. Tell me the story, not the business. And here I have no story. I have academic credentials at good schools, which tells me they're smart, but smart is table stakes. What I'm listening for is the biographical thread that makes this founder inseparable from this problem -- the way Travis Kalanick's intensity was inseparable from Uber, the way Crusoe's founders had independently arrived at an insight about waste gas that was rooted in something they'd actually seen. With Haladir, I can't find that thread. Maybe it exists and the public record simply doesn't capture it. But my entire evaluation framework starts with founder narrative, and when that channel is blank, I'm flying without my primary instrument.

The specific technical angle here does have a quality I find interesting -- the insight that classical operations research problems (vehicle routing, job scheduling, bin packing) represent an untapped source of verifiable training data for RL, distinct from the coding-centric approaches everyone else is building. That's a non-obvious observation, and the fact that most competitors are clustered around code tasks while Haladir is looking at solver-computed ground truth across ten OR domains suggests independent thinking. But I need to be honest about what kind of "non-consensus" this actually is. The RL training infrastructure space itself has become extremely hot money -- Applied Compute just raised $100M at a $1.3B valuation, Mechanize is paying engineers $500K to build environments for Anthropic, Scale AI has a dedicated RL team at a $29B valuation. Picking a different niche within a consensus category is not the same as backing something that makes people uncomfortable. When I invested in Contraline, male birth control made VCs squirm. When I wrote the check for SpaceX, people thought private space was lunacy. Haladir's angle is clever, but it doesn't trigger that instinct I've learned to trust -- the feeling that everyone else is wrong and this is going to look obvious in retrospect.

The product focus ambiguity concerns me more than the competitive landscape. The GitHub organization describes "AI-Enabled Mainframe Modernization and Code Translation," while the YC tagline is about bridging solvers and RL training. They have a COBOL modernization tool called Rosetta in request-access mode alongside an OR-bench for training data. I've seen this pattern before -- founders who are exploring multiple beachheads before committing -- and at pre-seed that's somewhat normal. But selling RL training environments to frontier AI labs and selling COBOL modernization to enterprises are fundamentally different go-to-market motions with different buyers, different sales cycles, and different credibility requirements. The founders I've backed who succeeded were almost maniacally focused on a single problem. The ones who hedged across multiple products often diluted their execution at exactly the moment they needed to be concentrated.

Let me engage honestly with the bull case, because I think there is one worth articulating. If the RL training paradigm continues to accelerate -- and Anthropic's reported $1B budget for environments suggests it will -- then the AI labs will need diverse verifiable domains beyond code. Operations research is a genuinely defensible domain because it requires simultaneous expertise in classical optimization, formal methods, and RL training pipelines. That's an unusual intersection. The fact that established OR companies like Gurobi and IBM aren't positioned to package their outputs as RL training data means there's a structural gap. And sometimes the youngest founders are the ones who see clearly because they have no institutional baggage. If these four students can establish OR-bench as a standard benchmark that frontier labs rely on, they could build a wedge into a massive market before the incumbents even realize the opportunity exists. Susa Ventures participating alongside YC suggests at least some experienced investors see something here.

But even in that scenario, I keep coming back to the question of who these founders are and why they're the right people for this specific problem. Selling to Anthropic and OpenAI requires extraordinary credibility. The Wing VC analysis in the dossier notes that "research credibility compounds -- trust, talent, and frontier exposure reinforce each other." Four undergrads with no published research, no industry experience, and no prior exits face a credibility gap that's particularly acute in this market. I don't see the economic access angle that pulls me toward companies -- this is infrastructure for already-powerful AI labs, not a tool that creates opportunity for people who currently lack it. And four co-founders from four different schools, without visible evidence of how they've navigated conflict together, reminds me of dynamics I've learned to probe carefully since HQ Trivia. I don't have red flags here -- I just don't have any signal at all on the dimension I care about most.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Founder Biographical Grit and Personal Stake | 7/30 |
| Anti-Consensus Conviction and Weird Factor | 13/25 |
| Economic Access and Real-World Impact | 4/20 |
| Navigating Complexity in Hard Spaces | 7/15 |
| Co-Founder Alignment and Team Resilience | 4/10 |
| **Total** | **35/100** |

**Total Score: 35/100** (Pass)
