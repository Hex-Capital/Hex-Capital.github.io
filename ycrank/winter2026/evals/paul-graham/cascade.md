# Cascade -- Paul Graham Evaluation

The first thing I notice about Cascade is that they're building in a space where there is no schlep blindness at all. AI agent safety is perhaps the most visible problem in technology right now. Gartner is publishing reports about it. Meta is open-sourcing guardrail frameworks. WitnessAI has raised $58 million. Fiddler AI has raised $100 million. F5 acquired CalypsoAI for $180 million. When I wrote about schlep blindness, the core observation was that the best startup opportunities hide in plain sight because the pain of solving them causes smart people to unconsciously look away. AI agent safety is the opposite — it's a problem that makes smart people unconsciously look toward it, because it sounds important, fundable, and prestigious. When a problem has this much gravitational pull, the question isn't whether it's real. The question is whether you can win it from a standing start with $500K when incumbents have 100-200x your capital.

The origin story here is academic, not organic. Both founders came through Berkeley AI Research Lab — AlSayyad studied agentic safety and graph reasoning, Demirhan worked on memory optimization for AI agents. AlSayyad did pen-testing of AI applications and researched AI voice security with Dawn Song. These are real credentials. But when I ask my first question — "how did they find this problem?" — the answer is essentially that they studied it in a research lab and saw a market forming. That's different from Drew Houston forgetting his USB drive or Brian Chesky not making rent. There's no moment of personal pain that made this problem impossible to ignore. It reads like two talented researchers surveyed the landscape, identified that agentic AI safety would be important, and decided to build a company. That's the kind of idea that emerges from deliberation rather than necessity. Founders who arrive at problems this way can succeed, but the motivation tends to be more fragile than founders who stumbled into something they couldn't stop thinking about.

The bull case deserves examination, because the timing argument is not trivial. If Gartner's prediction is right that 40% of enterprise applications will feature AI agents by end of 2026, the demand for safety infrastructure will be enormous — and it's possible that existing players like WitnessAI and Fiddler are architecturally wrong for autonomous agents because they were designed for the previous paradigm of LLM monitoring and model observability. Cascade's "self-improving" framing — models that learn from production agent failures rather than applying static rules — could represent a genuine architectural insight. And Demirhan's experience building production monitoring infrastructure at Netflix and Amazon is precisely the kind of background you'd want for building reliability infrastructure. If Cascade can get embedded in a few enterprise agent deployments early and build a data flywheel from observed failures, the compounding advantage could overcome the funding gap. This would be the DoorDash pattern — a crowded space where everyone assumed the category was saturated, but execution intensity and a subtly different approach proved decisive.

But I keep coming back to the competitive dynamics. This isn't a space where competition is structurally low because the schlep deters people. The schlep here is technical complexity, which is exactly the kind of challenge that attracts rather than repels AI researchers. Guardrails AI offers an open-source framework. Invariant Labs intercepts agent prompts with open-source tooling. Meta released LlamaFirewall. When the barrier to entry-level solutions is this low thanks to open source, your differentiation has to come from something very specific — the "self-improving" model quality, presumably — and I see no evidence that this exists yet. The website returned only CSS framework code at time of research. The GitHub repo returned a 404. There are zero traction signals of any kind: no users, no waitlist, no revenue, no beta testers, no press. At pre-seed I don't demand much, but I want to see something — a working prototype, a few design partners, a developer who tried it and told a friend. The absence of everything here, combined with a crowded consensus category, makes this a pass for me.

The founders are technically capable people — Berkeley CS, BAIR research, Netflix/Amazon infrastructure experience — and they've chosen a real problem in a growing market. I don't doubt their intelligence. But intelligence isn't what kills startups at this stage. What kills them is building something that was thought up rather than discovered, in a category where the competition is both well-funded and motivated. If these founders had come to me with a specific, ugly, regulatory or logistical problem they'd personally encountered while deploying AI agents in production — something nobody wanted to touch — I'd be much more interested. Instead, they've chosen the problem everyone wants to touch.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 8/30 |
| Relentlessly Resourceful Founders | 11/25 |
| Evidence of Wanting: Demonstrated User Pull | 6/20 |
| Technical Hacker Founders Who Build | 9/15 |
| Growth Trajectory and Default Alive Economics | 5/10 |
| **Total** | **39/100** |

**Total Score: 39/100** (Pass)
