# ARC Prize Foundation

> AI benchmarks that measure general intelligence and inspire new ideas

| Field | Value |
|-------|-------|
| Website | https://arcprize.org/ |
| YC Page | https://www.ycombinator.com/companies/arc-prize-foundation |
| Batch | Winter 2026 |
| Industry | Industrials / Industrials |
| Team Size | 4 |
| Location | Remote |
| Tags | |

## The Idea

**Problem:** Current AI benchmarks fail to reliably measure progress toward artificial general intelligence. Frontier AI models saturate existing benchmarks quickly (e.g., pure LLMs score 0% on ARC-AGI-2 while every task has been solved by at least 2 humans in under 2 attempts), leaving researchers, labs, regulators, and the public without a trusted yardstick for genuine reasoning capability (arcprize.org). The customer segments are AI labs evaluating model capabilities, policymakers assessing AI risk, and the open-source research community. Today, labs rely on fragmented benchmarks (MMLU, GPQA, HumanEval, etc.) that are prone to data contamination and do not specifically isolate fluid intelligence or skill-acquisition efficiency.

**Approach:** ARC Prize Foundation creates and stewards the ARC-AGI benchmark series — purpose-built evaluations of general intelligence that measure "skill acquisition efficiency" rather than memorized knowledge (arcprize.org). Each benchmark version is designed to resist brute-force approaches: ARC-AGI-2 is a static reasoning benchmark, while ARC-AGI-3 (launching March 25, 2026) introduces interactive reasoning environments with 1,000+ levels across 150+ environments requiring agents to explore, learn, plan, and adapt (arcprize.org). The Foundation pairs these benchmarks with annual public competitions on Kaggle (with $1M+ prize pools), research grants, a paper prize track, and an independent academic panel from UCLA, NYU, the Santa Fe Institute, and Columbia (arcprize.org).

**Differentiation:** Unlike general-purpose leaderboards (Hugging Face Open LLM Leaderboard, Scale AI SEAL), ARC-AGI is specifically designed to test novel reasoning on tasks never seen before, with human-calibrated difficulty. Unlike METR, which focuses on AI safety evaluations for dangerous capabilities, ARC Prize targets the measurement of *progress toward* AGI. Unlike Epoch AI, which aggregates performance across existing benchmarks, ARC Prize creates original evaluation tasks. Four frontier labs — OpenAI, Anthropic, Google DeepMind, and xAI — reported ARC-AGI performance in public model cards in 2025, a level of adoption that competing independent benchmarks have not achieved for AGI-specific measurement (arcprize.org/blog/arc-prize-2025-results-analysis).

**Business Model:** ARC Prize Foundation is a 501(c)(3) nonprofit that funds operations and prizes through tax-deductible donations (arcprize.org/donate). Named individual donors include Tyler Cowen, Andy Fang (DoorDash co-founder), Dharmesh Shah (HubSpot co-founder), and Aaron Levie (Box CEO). Lab donors include xAI, Google, Ndea, Prime Intellect, and Nous (arcprize.org). No pricing page or earned revenue model is publicly visible. [Inferred]: The most likely long-term sustainability path involves a combination of philanthropic grants, lab sponsorships, and government funding as AI regulation frameworks increasingly require independent evaluation infrastructure.

**TAM/SAM:** No public TAM/SAM data found for the AI benchmarking/evaluation segment as a standalone market. For context, the broader AI market was estimated at $371.71B in 2025 and projected to reach $2,407.02B by 2032 at 30.6% CAGR (MarketsandMarkets, 2025 via search snippet). The agentic AI market is projected at $7.06B in 2025 growing to $93.20B by 2032 at 44.6% CAGR (MarketsandMarkets, 2025 via search snippet). [Inferred]: The directly addressable market for AI evaluation infrastructure is a small fraction of these figures, but the organization's nonprofit structure means it competes for philanthropic and government funding rather than commercial market share.

**GTM / Distribution:** Distribution is anchored by the annual Kaggle competition (1,455 teams in 2025, 1,430 teams in 2024) (arcprize.org/blog/arc-prize-2025-results-analysis), an active Discord community (4,437 members) (discord.com/invite/9b77dPAmcA via search snippet), direct adoption by frontier labs as a reported benchmark, the @arcprize X account (26.2K followers) (x.com/arcprize via search snippet), and co-founder François Chollet's personal reach (608.1K X followers) (x.com/fchollet via search snippet). The Foundation also publishes technical reports on arXiv and produces community newsletters.

## Defensibility

**Benchmark authorship and IP:** ARC Prize controls the design, calibration, and release of the ARC-AGI benchmark series. Each version requires human testing for calibration — over 400 human participants were used to calibrate ARC-AGI-2 (arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025). François Chollet's authorship of the original ARC framework (published 2019) and the "On the Measure of Intelligence" paper provide intellectual continuity and credibility that cannot be replicated.

**Adoption as industry standard:** Four frontier AI labs voluntarily reporting ARC-AGI scores in model cards creates a network effect: once ARC-AGI becomes the reference benchmark, switching to an alternative imposes coordination costs across labs, researchers, and media (arcprize.org/blog/arc-prize-2025-results-analysis).

**Community and data:** Two years of competition data (2,885+ teams, 32,943+ entries, 130+ research papers across 2024-2025) create a growing corpus of solutions, techniques, and baselines that reinforces ARC-AGI's position as the research community's focal point (arcprize.org/blog/arc-prize-2025-results-analysis; arcprize.org/blog/arc-prize-2024-winners-technical-report).

**Market structure:** [Inferred]: AI labs have limited incentive to build their own AGI benchmarks because self-evaluation lacks credibility with regulators, media, and the public. The value of ARC-AGI to labs depends on its perceived independence — a structural dynamic that benefits a nonprofit benchmark provider over an internal lab effort. Scale AI operates SEAL benchmarks but has commercial relationships with labs that may create perceived conflicts of interest.

**Commoditization risk:** Benchmark design is not technically complex to replicate — any research group could publish a competing evaluation. However, the combination of community adoption, competition infrastructure, annual iteration cycle, independent academic panel, and the credibility of Chollet's framework creates meaningful switching costs. The primary commoditization risk comes from government-mandated evaluation frameworks (e.g., the U.S. AI Safety Institute selected Scale AI as a third-party evaluator in February 2025) (search snippet, TapTwice Digital) that could formalize alternative benchmarks into regulatory requirements.

## Market & Traction

**Traction signals:**
- 2024 competition: 1,430 teams, 17,789 entries, 40 research papers generated (arcprize.org/blog/arc-prize-2024-winners-technical-report)
- 2025 competition: 1,455 teams, 15,154 entries, 90 papers submitted, $162,500+ awarded (arcprize.org/blog/arc-prize-2025-results-analysis)
- ARC-AGI-1 SOTA increased from 33% to 55.5% during 2024 competition (arcprize.org/blog/arc-prize-2024-winners-technical-report)
- 4 frontier AI labs (OpenAI, Anthropic, Google DeepMind, xAI) reported ARC-AGI in public model cards in 2025 (arcprize.org/blog/arc-prize-2025-results-analysis)
- OpenAI's o3 scored 75.7% on ARC-AGI Semi-Private Eval; high-compute configuration scored 87.5% (x.com/arcprize, Dec 2024)
- Grok 4 (Thinking) achieved 15.9% on ARC-AGI-2, nearly doubling prior commercial SOTA (x.com/arcprize)
- Technical reports published on arXiv (arXiv:2412.04604, arXiv:2601.10904)
- X/Twitter @arcprize: 26.2K followers (x.com/arcprize via search snippet)
- Discord community: 4,437 members (discord.com via search snippet)
- LinkedIn: linkedin.com/company/arcprize (follower count not retrievable)
- Press coverage: TechCrunch (Jan 2025), IEEE Spectrum (2 articles)
- ARC-AGI GitHub repo: 4.7K stars (github.com/fchollet/ARC-AGI via search snippet)
- No Product Hunt listing found

**Competitive landscape:**

| Competitor | Funding | Revenue/ARR | Key Differentiator vs. ARC Prize |
|---|---|---|---|
| **Scale AI (SEAL)** | $15.9B total raised, $29B valuation (Sacra, 2025 via search snippet) | ~$2B projected 2025 (TapTwice Digital via search snippet) | For-profit; broad AI evaluation platform (quality, safety, alignment); commercial relationships with labs may create conflict of interest; selected as U.S. AISI third-party evaluator |
| **METR** | ~$38M catalyzed through Audacious Project for Canary collaboration (metr.org, Oct 2024) | Nonprofit, no public revenue | Focuses on AI safety evaluations for dangerous capabilities, not AGI progress measurement; does not run public competitions |
| **Epoch AI** | Raised $10M+ in 2025 (Epoch AI impact report, 2025) | Nonprofit with some paid services, no public revenue figure | Aggregates and indexes existing benchmarks; produces research on AI trends; does not create original AGI-specific evaluation tasks |
| **Artificial Analysis** | $250K raised (startup-seeker.com via search snippet) | No public data found | Focuses on model performance/price/speed comparison for commercial decision-making, not AGI measurement |
| **Hugging Face (Open LLM Leaderboard)** | $395M total raised (via search snippet) | No public revenue figure | Hosts community-contributed benchmarks and leaderboards; broader scope as ML platform; Yourbench allows custom evaluation creation |

**Why now:** [Inferred]: Three converging developments opened this opportunity: (1) the emergence of "reasoning" models (OpenAI o1/o3, DeepSeek-R1, Gemini 3 Deep Think) in 2024-2025 created a new class of AI systems that existing benchmarks like MMLU quickly saturate, creating demand for harder, reasoning-specific evaluations; (2) increasing government interest in AI regulation (U.S. AI Safety Institute, EU AI Act) is generating demand for independent, trusted evaluation infrastructure; (3) François Chollet's departure from Google in late 2024 (The Decoder, Nov 2024) enabled full-time focus on formalizing ARC Prize as an institution.

## Founders & Team

**Greg Kamradt** — Founder / President
- Ex-Salesforce engineering director; previously Director of Strategy & Growth leading a team of 8 (gregkamradt.com/about)
- First business hire at Digits (fintech startup) (gregkamradt.com/about)
- Founder of Leverage, an AI product studio (gregkamradt.com)
- Creator of the "Needle in a Haystack" LLM context retrieval test, widely adopted by the AI community (~1.5K GitHub stars) (github.com/gkamradt/LLMTest_NeedleInAHaystack via search snippet)
- Education: Zipfian Data Science Academy (2015) (gregkamradt.com/about)
- Twitter/X: @GregKamradt — 46.8K followers (x.com/GregKamradt via search snippet)
- LinkedIn: linkedin.com/in/gregkamradt (headline not retrievable)
- GitHub: github.com/gkamradt — 38 repositories; LLMTest_NeedleInAHaystack (~1.5K stars) (via search snippet)

**Mike Knoop** — Co-founder
- Co-founder of Zapier (YC S12), valued at $5B (TapTwice Digital via search snippet); Zapier generated $310M revenue in 2024 (TapTwice Digital via search snippet)
- At Zapier: led all product and engineering, then shifted to Head of AI R&D in 2022; launched Zaps, Transfer, Tables & Interfaces, AI Actions, Central; led OpenAI partnership (mikeknoop.com)
- BS Mechanical Engineering, University of Missouri (mikeknoop.com)
- Twitter/X: @mikeknoop — 23.4K followers (x.com/mikeknoop via search snippet)
- LinkedIn: linkedin.com/in/mikeknoop (headline not retrievable)
- GitHub: github.com/mikeknoop — 53 followers (via search snippet)

**François Chollet** — Co-founder
- Creator of Keras, one of the most widely used deep learning frameworks (63.8K GitHub stars, 2M+ users) (github.com/keras-team/keras via search snippet; Wikipedia)
- Former Google AI researcher for 10 years (joined 2015, departed late 2024) (Wikipedia; The Decoder, Nov 2024)
- Author of the original ARC-AGI benchmark (2019) and "On the Measure of Intelligence" paper (Wikipedia)
- Author of "Deep Learning with Python" (O'Reilly) (fchollet.com)
- Author of Xception (depthwise separable convolutions), 18,000+ citations (Wikipedia)
- Diplôme d'Ingénieur (Master of Engineering), ENSTA Paris / Polytechnic Institute of Paris (Wikipedia)
- Research at University of Tokyo (2012) (PyImageSearch interview)
- Twitter/X: @fchollet — 608.1K followers (x.com/fchollet via search snippet)
- LinkedIn: No public profile URL found in search
- GitHub: github.com/fchollet — 17.8K followers; ARC-AGI repo: 4.7K stars; Keras: 63.8K stars (via search snippet)

**Co-founder relationship:** Mike Knoop and Greg Kamradt both operate in the San Francisco AI community and have backgrounds that overlap in AI product development. Knoop (Zapier, YC S12) and Chollet (Google, Keras) connected through Chollet's ARC-AGI benchmark work. No shared prior employer or university identified among the three founders.

**Founder-market fit:** The founding team combines deep technical credibility in AI evaluation (Chollet created the benchmark itself and Keras), large-scale product and platform-building experience (Knoop scaled Zapier to $5B valuation and led AI R&D), and AI community building and evaluation methodology expertise (Kamradt created the widely adopted "Needle in a Haystack" LLM test and has a 46.8K-follower audience in the AI practitioner community). Chollet's 608.1K X following and authorship of the foundational ARC framework give the organization access to the global AI research community. Notable individual donors include Andy Fang (DoorDash co-founder), Dharmesh Shah (HubSpot co-founder), and Aaron Levie (Box CEO) (arcprize.org).

## Key Risks

**Benchmark saturation cycle:** Each ARC-AGI version has a finite lifespan as AI systems improve. ARC-AGI-1 went from 33% to 55.5% SOTA within a single competition year (2024), and OpenAI's o3 scored 87.5% on the semi-private eval shortly after (x.com/arcprize, Dec 2024). The Foundation must continuously invest in designing harder benchmarks (ARC-AGI-2, 3, and beyond) to remain relevant. If the pace of AI capability gains outstrips the benchmark design cycle, ARC-AGI could lose its differentiating difficulty.

**Nonprofit funding dependency:** The organization relies entirely on donations from individuals and AI labs to fund operations and prize pools. Lab donors (xAI, Google) are also the entities being evaluated, creating a potential perception of conflict of interest. A withdrawal of lab funding or donor fatigue could constrain the Foundation's ability to maintain competitive prize pools ($1M+/year) and operations.

**Regulatory capture by alternatives:** The U.S. AI Safety Institute selected Scale AI as a third-party evaluator in February 2025 (TapTwice Digital via search snippet). If government regulators formalize alternative benchmarks into mandatory evaluation frameworks, ARC-AGI could be sidelined from the regulatory evaluation stack despite its research community adoption.

**Format migration risk with ARC-AGI-3:** ARC-AGI-3 introduces a fundamentally different interactive format (game environments requiring exploration and adaptation) compared to the static grid-based tasks of ARC-AGI-1 and 2 (arcprize.org). This format change could fragment the community if researchers optimized for static reasoning do not transition, or if the new format proves less clearly correlated with general intelligence than the original.

**Key-person dependency on Chollet:** The intellectual foundation and credibility of the ARC-AGI framework are closely identified with François Chollet personally. His departure or reduced involvement could diminish the perceived authority of future benchmark versions.

## Key Facts

| Dimension | Data |
|-----------|------|
| TAM | No public data found for AI benchmarking/evaluation as standalone market. Broader AI market: $371.71B in 2025, projected $2,407.02B by 2032 at 30.6% CAGR (MarketsandMarkets, 2025 via search snippet) |
| SAM | No public data found |
| Traction | 1,455 teams / 15,154 entries in 2025 (arcprize.org); 1,430 teams / 17,789 entries in 2024 (arcprize.org); 4 frontier labs report ARC-AGI in model cards (arcprize.org); ARC-AGI GitHub repo 4.7K stars (GitHub); Discord 4,437 members (Discord via search snippet) |
| Revenue Signal | Nonprofit; no earned revenue model. $1M+ annual prize pool funded by donations. Named donors: Andy Fang, Dharmesh Shah, Aaron Levie, Tyler Cowen; lab donors: xAI, Google, Ndea, Prime Intellect, Nous (arcprize.org) |
| Founders | Greg Kamradt (President): ex-Salesforce director, creator of Needle in a Haystack LLM test. Mike Knoop (Co-founder): co-founder of Zapier ($5B valuation), YC S12, BS Mech. Eng. UMissouri. François Chollet (Co-founder): creator of Keras (63.8K GitHub stars), ex-Google 10yr, ENSTA Paris |
| Competitors | Scale AI SEAL ($15.9B raised, ~$2B projected 2025 revenue, broad AI eval platform); METR (~$38M via Audacious Project, nonprofit, AI safety focus); Epoch AI ($10M+ raised 2025, nonprofit, benchmark aggregation); Artificial Analysis ($250K raised, commercial model comparison); Hugging Face ($395M raised, Open LLM Leaderboard) |
| Moat Signals | ARC-AGI reported in model cards by 4 frontier labs (OpenAI, Anthropic, Google DeepMind, xAI); Chollet's authorship of foundational framework; 2,885+ competition teams across 2 years; independent academic panel |
| Risk Factors | Benchmark saturation cycle requiring continuous redesign, nonprofit funding dependency on entities being evaluated, regulatory capture by Scale AI/government alternatives |
| Founder Reach | Greg Kamradt: Twitter 46.8K, GitHub ~1.5K stars (NeedleInAHaystack). Mike Knoop: Twitter 23.4K, GitHub 53 followers. François Chollet: Twitter 608.1K, GitHub 17.8K followers + 4.7K stars (ARC-AGI) + 63.8K stars (Keras) |
| Distribution Signals | Kaggle competition (1,455 teams, 2025); Discord 4,437 members; X @arcprize 26.2K followers; IEEE Spectrum coverage (2 articles); TechCrunch coverage (Jan 2025); arXiv technical reports; no Product Hunt listing found |
