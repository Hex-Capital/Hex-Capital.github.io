# Corelayer -- Elad Gil Evaluation

The competitive landscape here tells me a lot about where we are in this market's cycle. Resolve AI raised $125M at a $1B valuation in December 2025. incident.io closed a $62M Series B at $400M. BigPanda has taken in $431M. PagerDuty is a $440M-revenue public company. This is not a non-obvious market -- it's a market where serious capital has already arrived and category leaders are consolidating. When I invested in PagerDuty, operations alerting was genuinely boring -- most investors dismissed it as niche tooling for a small number of SREs. That's no longer the case for AI-powered incident management. Everyone sees this market now, which means Corelayer is entering a well-funded arena as a 2-person team with no disclosed revenue or customers. The "why now" in the dossier amounts to "LLM capabilities crossed a threshold in 2024-2025" -- but that's the same timing thesis powering hundreds of AI agent companies. There's no specific capability curve I can track the way I tracked GPT scaling for Harvey. The structural moment for AI-powered ops tools has already been identified and capitalized by incumbents and well-funded startups alike.

The one angle that keeps me from a clean pass is the regulated-industry wedge with on-prem and confidential compute. I've seen this pattern work -- regulatory barriers create structural moats that cloud-native companies genuinely struggle to breach. Datadog's entire business model is cloud SaaS; rebuilding for on-prem deployment with confidential compute isn't just an engineering problem, it conflicts with their margin structure and go-to-market motion. This is similar to why Anduril found open space in defense -- ITAR compliance and security clearances created a structural barrier that incumbents either couldn't or wouldn't cross. For Corelayer, the argument would be: financial services and healthcare companies that cannot send production data to the cloud need AI debugging agents that run inside their own environments, and nobody well-funded is building specifically for that constraint. That's a narrower but potentially defensible position. The SOC 2 Type I certification shows the founders understand this compliance-first sales motion isn't optional.

The founders have genuine domain fit. Both built data infrastructure at Goldman Sachs, working on systems processing hundreds of billions of rows daily. They experienced the pain of data-specific production incidents -- missing rows, duplications, incorrect values -- that general infrastructure monitoring misses entirely. Shipra Jha's CMU CS background and Oracle cloud infrastructure experience add technical depth. This isn't founders who pivoted into a trendy sector; they lived inside this problem at one of the most demanding data environments in financial services. The 20+ integrations (GCP, AWS, Snowflake, Datadog, Airflow, dbt) and SOC 2 certification suggest real building velocity for a two-person pre-seed team. That's operational seriousness.

But here's where I apply the single-miracle test and get concerned. The core miracle -- can AI agents accurately diagnose data pipeline issues in production? -- is identifiable and partly being delivered by foundation model labs, which is a favorable Harvey-like dynamic. However, there's a second obstacle: enterprise sales in regulated industries with a 2-person team. Fortune 100 financial services procurement cycles run 12-18 months with extensive security reviews and vendor assessments. Getting from YC demo day to a signed Fortune 100 contract requires navigating procurement bureaucracy that is its own low-probability challenge. And then there's a third: competing against Resolve AI's $160M war chest and incident.io's established customer base. That's approaching multi-miracle territory. Each obstacle is individually surmountable, but compounding them starts to feel like the planning requires too many things to go right sequentially.

The bull case I'd need to believe: Corelayer's data-content-aware debugging layer represents a genuinely new category that existing incident management and observability platforms cannot absorb as a feature -- and the on-prem/confidential compute requirement in regulated industries creates enough structural separation that Resolve AI and Datadog can't follow easily. If the founders can land two or three marquee financial services customers through their Goldman network, the switching costs from the AI's accumulated knowledge of customer-specific data pipeline architectures could be substantial. The "boring but critical" archetype applies -- every bank and insurer runs data pipelines, they all have data quality incidents, and nobody has built a debugging agent specifically for this context. If I believed the competitive moat was deep enough, this would be the kind of boring, regulation-heavy infrastructure bet I've made money on repeatedly. But the individual technical components -- log monitoring, anomaly detection, LLM-powered analysis -- are reproducible, and the well-funded competitors are one product roadmap decision away from adding data-level debugging. The moat needs to be the deployment model and compliance posture, not the AI itself, and I'm not convinced that's wide enough against this level of competitive capital.

The product-to-distribution trajectory is unclear. The initial product could theoretically expand into broader data quality monitoring, compliance automation, or operational analytics -- the customer relationship in regulated environments is inherently sticky. But there's no evidence the founders have articulated this expansion. Right now, this reads as a single-purpose debugging tool, not a platform that becomes a distribution channel. For a company I'm going to invest in at pre-seed, I need to see at least the seeds of platform thinking, even if execution is years away.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Non-Obvious Market at Structural Inflection | 14/35 |
| Product-to-Distribution Trajectory | 11/25 |
| Single-Miracle Operational Clarity | 7/15 |
| Founder Execution Velocity | 8/15 |
| Technology Cycle Positioning | 6/10 |
| **Total** | **46/100** |

**Total Score: 46/100** (Neutral)
