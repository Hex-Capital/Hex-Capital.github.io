# Cascade -- Naval Ravikant Evaluation

The first thing I notice about Cascade isn't the founders or the technology -- it's the market structure. AI safety infrastructure is one of the most consensus categories in enterprise software right now. WitnessAI has raised $58 million. Fiddler AI has raised roughly $100 million. F5 acquired CalypsoAI for $180 million. Gartner is publishing reports about "guardian agents" and "AI security platforms." Meta is open-sourcing LlamaFirewall. Every pitch deck in enterprise AI has a slide about agent safety. This is the exact opposite of my best investments. When I backed Uber at a $4 million valuation, ride-sharing was illegal. When I bought Bitcoin in 2014, serious people called it a Ponzi scheme. The asymmetric upside in angel investing comes from being non-consensus and right. Cascade is entering a space where the consensus has already formed, the capital has already arrived, and the pricing already reflects optimism. That's a structural problem no amount of execution can fix.

The specific knowledge question is where I start every evaluation, and here the answer is: relevant but replicable. AlSayyad researched at BAIR with Dawn Song on AI voice security and did pen-testing of AI-integrated applications. Demirhan built production monitoring infrastructure at Netflix and Amazon and researched memory optimization for AI agents at BAIR. These are strong credentials -- better than most pre-seed teams in this space. But credentials are not specific knowledge. Specific knowledge is what you learn through years of obsessive personal curiosity about a domain -- the kind of understanding that produces insights that surprise even experts. What I see here is two talented Berkeley graduates who identified a hot category and have the right resume bullets to pursue it. That's different from someone who has been living inside a problem so long that they see patterns invisible to everyone else. The Dawn Song connection is notable, but one research collaboration doesn't constitute the kind of deep, non-transferable understanding I look for. Compare this to how the Notion founders spent years thinking about the relationship between tools and thought itself -- that depth of domain immersion is qualitatively different from "I researched AI safety at a good lab."

The leverage question is more interesting but ultimately unresolved. The "self-improving safety models" framing implies a data flywheel: each production deployment teaches the system new failure modes, making it more valuable for all customers. If that flywheel actually works, it creates genuine leverage -- one engineering team's work compounds across every customer's agent deployment. That's the infrastructure play I love, the picks-and-shovels bet. Stack Overflow worked this way: every question and answer made the knowledge graph harder to replicate. But here's the problem -- there's no evidence the flywheel exists. The GitHub repo returned a 404. The website returned only CSS framework code. No technical artifacts are publicly visible. I need to see a technologist building, not just credentialed to build. The self-improving claim is a pitch concept, not demonstrated technology. And the competitive landscape includes teams with ten to twenty times more capital building overlapping capabilities.

The strongest bull case I can construct: guardian agents for autonomous AI could be the defining infrastructure layer of the agentic era, and Cascade's combination of BAIR research and Netflix/Amazon production experience is genuinely well-suited for this specific problem. If agentic AI adoption follows Gartner's projections -- 40% of enterprise applications by end of 2026 -- the demand for safety infrastructure will be enormous. Cascade could build a proprietary data advantage from early production deployments that becomes an unchallengeable moat. And the conflict of interest between AI platform providers selling agents and auditing their safety could create structural space for an independent safety layer. For this to be a great investment, all of those things would need to be true simultaneously, and Cascade would need to ship differentiated technology fast enough to matter before better-funded competitors or the platform providers themselves close the window.

But that bull case collides with a structural reality Naval's framework is built to detect: when the platform owns the distribution, standalone infrastructure between the platform and the user is vulnerable. OpenAI, Anthropic, and Google have every incentive to make their agents safe -- unsafe agents threaten their core business. This isn't like cybersecurity where the cloud providers had structural reasons to leave security to third parties. The AI providers want to own safety because it's core to their product promise. That means Cascade is building a layer that the platforms above it are actively trying to absorb. The analogy isn't CrowdStrike securing cloud workloads -- it's building an antivirus for an operating system whose maker is shipping antivirus built in.

I respect the founders' technical backgrounds, and the problem they're addressing is real. But through my framework, this is a competent team pursuing a consensus idea in a crowded market with well-funded incumbents and platform commoditization risk. The specific knowledge isn't distinctive enough. The leverage architecture is hypothetical. The timing is consensus, not contrarian. I need at least one of those three pillars to be exceptional to write a check. None of them are.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Specific Knowledge and Founder-Problem Authenticity | 14/30 |
| Leverage Architecture and Scalability of the Model | 13/25 |
| Contrarian Positioning and Non-Consensus Timing | 6/20 |
| Founder Integrity and Long-Term Orientation | 8/15 |
| Technical Compounding and Defensibility Over Time | 5/10 |
| **Total** | **46/100** |

**Total Score: 46/100** (Neutral)
