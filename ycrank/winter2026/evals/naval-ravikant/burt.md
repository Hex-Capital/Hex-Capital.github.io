# Burt -- Naval Ravikant Evaluation

The most revealing thing about Burt isn't what the founders are building -- it's that they appear to be building two different companies simultaneously. The YC page describes a horizontal fine-tuning platform. The website describes logistics back-office automation. These aren't two go-to-market strategies for the same product. They're two fundamentally different businesses requiring two fundamentally different types of specific knowledge. When a founder has genuine domain insight -- the kind that emerges from years of obsessive curiosity about a particular problem -- they don't oscillate between "infrastructure for everyone" and "automation for freight brokers." Clarity of vision is the byproduct of specific knowledge. Confusion about what you're building is usually the absence of it.

Bobby Zhong is a smart, energetic young engineer. UC Irvine CS, founding engineer at two YC companies, hands-on with LLMs at Replo. That's a solid trajectory for a 2026 software engineer. But when I ask the foundational question -- what does Bobby know that couldn't be taught to any competent ML engineer in six months? -- the answer isn't clear. He hasn't spent years researching model distillation or fine-tuning methodology. He doesn't appear to have a logistics background that would give him non-transferable insight into freight document workflows. He's building in the space where he most recently worked, which is how career optimization produces startups, not how specific knowledge produces them. Kurt Sharma, the CTO, has essentially no public footprint -- no GitHub, no confirmed LinkedIn, no published work. For a company whose entire value proposition is technical superiority in model training, the inability to externally validate the CTO's depth is a problem, not a gap in research.

The leverage architecture here concerns me. The primary call-to-action on the website is booking a 30-minute meeting with Bobby via Cal.com. The YC description says "we've helped teams build" specialized models -- past tense, consultative framing. This reads like a managed service, not a platform. Managed services scale linearly with headcount. Every new customer requires human attention, custom training runs, bespoke optimization. That's not leverage -- that's renting out your time with a technical veneer. The structural insight buried in the dossier -- that cloud providers are incentivized to maximize compute consumption while Burt's value proposition is minimizing it -- is genuinely interesting as an alignment gap. But identifying a misalignment and building a leveraged business that exploits it are different things. Right now, Burt appears to be doing the former without the latter.

The strongest bull case would require believing that the logistics pivot represents emergent specific knowledge -- that Bobby discovered something non-obvious about document processing in freight operations through early customer work, and the positioning confusion is just the messy reality of a pre-seed company finding its footing. If Burt accumulates proprietary training data and fine-tuning recipes across dozens of logistics customers, that corpus could compound into a defensible advantage. The Predibase acquisition by Rubrik validates demand for fine-tuning capabilities while removing a funded competitor. And the compute misalignment with cloud providers is a real structural opening. But this bull case requires several things to be true simultaneously that have no evidence yet: that the founders have logistics-specific insight, that the managed service can be productized into a scalable platform, and that the fine-tuning recipes compound rather than commoditize. Each of those is a separate leap of faith.

The timing and competitive positioning work against this bet. Fine-tuning specialized models is 2024-2025 consensus -- every enterprise AI team knows general-purpose models are too expensive at scale. Together AI has $534M and ~$300M in ARR. Fireworks AI has $327M and 10,000+ customers. Open-source tools like Unsloth and Axolotl make fine-tuning accessible to any ML engineer with a GPU. There's no contrarian insight here. "Specialized models outperform general-purpose ones for specific tasks" is a PowerPoint slide in every AI strategy deck. And the technology doesn't compound in any distinctive way -- the fine-tuning toolchain is rapidly commoditizing, with no patents, no proprietary datasets, no open-source community building around Burt's approach. When I invested in Stack Overflow, the knowledge graph got more valuable with every question and answer -- each interaction strengthened the moat. With Burt, each customer engagement improves the team's consulting expertise, but that's human capital, not compounding technology.

I pass. The founders have energy and technical competence, but competence in a consensus category without specific knowledge is how you build a company that gets acqui-hired, not one that compounds for a decade. I need the founder to know something that surprises me about their domain -- something I couldn't learn from reading three blog posts about LoRA fine-tuning. That signal isn't here.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Specific Knowledge and Founder-Problem Authenticity | 6/30 |
| Leverage Architecture and Scalability of the Model | 9/25 |
| Contrarian Positioning and Non-Consensus Timing | 5/20 |
| Founder Integrity and Long-Term Orientation | 8/15 |
| Technical Compounding and Defensibility Over Time | 3/10 |
| **Total** | **31/100** |

**Total Score: 31/100** (Pass)
