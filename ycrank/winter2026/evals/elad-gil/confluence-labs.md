# Confluence Labs -- Elad Gil Evaluation

The most striking thing about Confluence Labs is the gap between what they've demonstrated and what they'd need to become a business. A 97.9% score on ARC-AGI-2 -- where the best commercial systems hit 37.6% in the ARC Prize competition -- is a genuinely exceptional technical result. Two early-career founders achieved something on abstract reasoning benchmarks that frontier labs with billions in compute hadn't. Then they open-sourced it under MIT license and put up a website that says they're "looking for collaborators." That sequence tells me everything about where this company is: technically impressive, commercially unformed.

Start with the market's structural moment. "AI for science" is not a non-obvious market -- it's one of the most consensus-hot categories in venture capital right now. Lila Sciences raised a $200M seed. Periodic Labs raised $300M. CuspAI raised $100M Series A. Poetiq raised $45.8M. All in 2025. When I see five well-funded competitors and multiple hundred-million-dollar raises in a single year, that's a market where the structural inflection has been widely recognized and capital has already flooded in. The question isn't whether AI will transform scientific R&D -- it's whether a two-person pre-seed team with no domain-specific expertise in pharma, materials science, or biology can capture value in a market where Recursion Pharmaceuticals runs millions of biological experiments weekly and Citrine has 12 years of materials data accumulation. The "data efficiency" angle adds a twist, but it doesn't make this a non-obvious market. It makes it a non-obvious approach within a very obvious market, which is a fundamentally different and weaker position.

The multi-miracle problem is severe here. To succeed, Confluence Labs needs to: (1) prove that abstract benchmark performance on grid puzzles translates to real-world experiment design in chemistry, biology, or materials science -- domains with fundamentally different structure than ARC-AGI-2 tasks; (2) build domain-specific credibility and datasets in at least one vertical, without any existing domain expertise on the team; (3) develop an enterprise sales motion for pharma and materials R&D organizations, which have notoriously long procurement cycles and require deep technical trust; and (4) outcompete teams with 10-100x their capital and existing customer relationships. That's at least three miracles, probably four. I've written explicitly about why compounding low-probability events kill companies -- and the path from "we solved a benchmark" to "enterprise R&D teams pay us to design their experiments" requires clearing each of these hurdles sequentially.

The distribution picture compounds my concern. There is no product. No pricing. No customers. No pilots. No GTM strategy beyond soliciting collaborators via a contact email. Worse, they've open-sourced their primary technical asset, which means any team with access to frontier LLMs can reproduce the methodology. I understand the credibility-building logic of open-sourcing, but it actively undermines the defensibility that would justify an investment. When I evaluate whether a company can become a distribution channel for additional products -- which is how good companies become generational ones -- I see nothing here. Being "domain-agnostic" in scientific R&D sounds like platform potential, but in practice it usually means you're not deep enough in any single domain to be trusted by the buyers who matter.

The strongest bull case: Baskaran and Burdick have demonstrated a rare capability in sample-efficient AI reasoning, and if that capability generalizes from abstract benchmarks to physical-world experiment design, it could become foundational infrastructure for the entire AI-for-science wave. The Webvan-to-Instacart pattern might apply -- previous attempts at AI-driven experiment design failed because the models couldn't reason well enough with sparse data, and now they can. If you believe "data-efficient modeling" is a capability layer rather than a product, and that the founders will find the right domain wedge through their collaborator outreach, the ARC-AGI result is evidence of the kind of deep technical moat that early Stripe had in payment processing. The technical execution -- two people, no institutional backing, outperforming frontier labs on a hard benchmark -- is genuinely impressive for pre-seed. But for this bull case to hold, the benchmark-to-product translation needs to be one miracle, not three. And the open-sourcing needs to have been a calculated distribution strategy rather than a default research-lab behavior. I don't see evidence of either.

The founders show real technical talent -- Baskaran's RSI and ISEF credentials plus Burdick's prolific shipping history across 6+ startup MVPs create a credible research-plus-builder pairing. The benchmark result itself is evidence of rapid technical execution. But selling AI-driven experiment design to pharmaceutical R&D teams requires domain credibility, regulatory understanding, and enterprise sales capability that neither founder has demonstrated. This is fundamentally different from selling developer tools where the buyer evaluates the product by using it. In my experience building Color Health -- navigating CLIA regulations and selling genetic testing to healthcare systems -- domain credibility in regulated science takes years to accumulate. It's not something you bootstrap through benchmark scores.

This is a technically talented team in a consensus market, with no product, no distribution moat, a multi-miracle execution path, and well-funded competitors who are already further along in every relevant dimension. The core capability is real but unproven outside synthetic benchmarks, and the primary technical asset has been given away. I pass.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Non-Obvious Market at Structural Inflection | 10/35 |
| Product-to-Distribution Trajectory | 5/25 |
| Single-Miracle Operational Clarity | 4/15 |
| Founder Execution Velocity | 7/15 |
| Technology Cycle Positioning | 6/10 |
| **Total** | **32/100** |

**Total Score: 32/100** (Pass)
