# AutoSitu -- Paul Graham Evaluation

Municipal development review is one of those problems that's been staring at every developer, architect, and city planner in America for decades. Everyone who has ever waited nine months for a building permit knows the system is broken. But almost nobody tries to fix it from the inside -- from the city reviewer's desk -- because the schlep is staggering. You'd need to understand zoning codes that vary by jurisdiction, parse architectural drawings across multiple formats, coordinate between planning, fire, and engineering departments that barely talk to each other, and then sell the result to government agencies with 12-month procurement cycles. That's four layers of pain stacked on top of each other. Most founders unconsciously filter this out before it even registers as an opportunity. What catches my attention about AutoSitu is that Xuanshu Lin didn't filter it out, because he was already inside the system. He studied urban planning at Michigan and Harvard GSD, worked on municipal planning projects in Champaign-Urbana and Detroit, and presumably sat through enough review cycles to understand exactly where the process breaks down. This isn't someone who read about the housing crisis and decided to build an AI tool. This is someone who lived in the planning world and built outward from that experience.

The organic discovery here is strong. Lin's path through urban planning education and into municipal project work gives him the kind of domain knowledge you can't acquire from market research. He knows what a zoning compliance check actually involves, which code sections reviewers trip over, and why resubmittals happen. Pair that with George Zhai's background in autonomous systems and robotics from Georgia Tech -- someone who knows how to build AI systems that parse complex real-world inputs -- and you get a founding team where each person brings something the other can't replicate. The combination maps directly onto the product's dual challenge: understanding municipal code (Lin) and automating document analysis (Zhai). That said, I don't see evidence that they worked together before AutoSitu. Lin was at Michigan and Harvard; Zhai was at Georgia Tech. The co-founder relationship is a gap I'd want to probe. I've seen too many teams where founders met specifically to start a company and fell apart within a year.

The traction signal that stands out is the eight named city clients: San José, Seattle, Long Beach, Boston, Los Angeles, Dallas, San Francisco, Honolulu. For a two-person pre-seed company selling into government, getting your foot in the door of eight major metros is not trivial. Government buyers don't take meetings with companies they're not interested in -- their time is too constrained. Whether these are paid contracts, pilots, or evaluations matters enormously but isn't disclosed. The 1,500+ plan sets checked suggests the product is functional and being used, not vaporware. But I notice no revenue data anywhere, which in the context of government sales could mean they're giving the product away to build case studies. That's a legitimate strategy at this stage, but it means the evidence of wanting is real but shallow -- cities are using it, but we don't know if they'd pay for it.

The bull case here is genuinely compelling. If AutoSitu can build a defensible Code & Precedent Graph that deepens with each jurisdiction onboarded, they create a data asset that's extremely expensive to replicate. Every new city generates jurisdiction-specific training data and precedent patterns. Municipal switching costs are structurally high -- once a city department integrates your tool into their review workflow and trains staff on it, ripping it out is politically and operationally painful. And the positioning on the municipal reviewer side (as opposed to PermitFlow's applicant side) means they're embedded in the actual bottleneck. If cities start to depend on this for their daily review work, AutoSitu becomes infrastructure rather than a nice-to-have. The political tailwinds are real too -- housing shortage, understaffed planning departments, state governments actively pushing for permitting technology. This is a moment when government buyers are actually motivated to adopt.

What gives me pause is the competitive landscape. PermitFlow has raised $90.5 million and could expand from the applicant side into municipal review. CivCheck got acquired by Clariti, giving it resources. Blitz AI, CodeComply.AI, Cembla -- the category is filling up. When I see five or six funded companies attacking adjacent pieces of the same problem, that's usually a sign that the opportunity has been identified by the market, and the question becomes pure execution. AutoSitu's differentiation -- multi-agent coordination across departments, full review lifecycle rather than just pre-screening -- is real but not necessarily durable. The core technical capability of extracting structured data from drawings and checking against codes is buildable by well-resourced teams. The moat has to come from the jurisdiction-specific data and the depth of municipal relationships, not the AI itself. Whether a two-person team can accumulate that data faster than competitors with ten or fifty times their funding is the central bet.

I'd want to invest enough time with Lin and Zhai to understand two things: how they found each other and whether the partnership is deep enough to survive the grinding work ahead, and whether the city clients listed on the website represent real operational integration or early-stage conversations dressed up as partnerships. If the cities are actually running plan sets through the system as part of their daily workflow, this is a much stronger company than if they're in evaluation mode. The organic problem discovery and the schlep willingness are there. The question is whether the founders have the staying power and resourcefulness to navigate government sales at scale while a competitor with ninety million dollars watches from the adjacent lane.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 23/30 |
| Relentlessly Resourceful Founders | 14/25 |
| Evidence of Wanting: Demonstrated User Pull | 13/20 |
| Technical Hacker Founders Who Build | 9/15 |
| Growth Trajectory and Default Alive Economics | 5/10 |
| **Total** | **64/100** |

**Total Score: 64/100** (Neutral)
