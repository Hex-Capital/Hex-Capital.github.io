# Compresr -- Elad Gil Evaluation

The first thing I notice about Compresr is that it's building a cost-reduction middleware layer on top of a platform whose costs are declining ~10x annually. That's the central tension. LLM inference prices have been on one of the steepest deflationary curves in technology history -- Anthropic already ships prompt caching, OpenAI offers cached context discounts, and every major provider is racing to cut per-token pricing. Compresr is essentially arbitraging a cost that the platform owners are actively compressing through other means. I've seen this pattern before: it's structurally similar to building a data compression startup for cloud storage in 2010. The arbitrage window closed not because compression stopped working, but because storage got cheap enough that most users stopped caring. The question for Compresr is whether the agentic AI explosion generates enough absolute token spend to outrun the deflationary curve. That's a real possibility -- model API spending doubled to $8.4B even as unit costs cratered -- but it means the company is in a footrace against platform economics, and I generally avoid businesses whose core value proposition depends on someone else's pricing staying high enough to matter.

The "why now" argument has real components but doesn't clear my bar for a structural inflection. Yes, agentic workflows consume 10-100x more tokens per session than simple prompts, and yes, context windows have expanded to 128K-1M+ tokens where filling them proportionally increases cost and can degrade accuracy. But the providers aren't standing still. Anthropic's prompt caching, OpenAI's cached context pricing, and the consistent trend toward cheaper-and-longer context windows are all moves that erode the compression value proposition without directly competing with it. LLM providers face a structural disincentive to offer compression per se -- it cannibalizes their per-token revenue -- but they have every incentive to offer alternatives that achieve equivalent outcomes for their customers. That's a subtle but critical distinction. The market for "making LLM inference cheaper" is real, but it's being addressed from multiple directions simultaneously, and a third-party compression layer is only one approach.

The strongest bull case I can construct: Compresr could become the Cloudflare of LLM inference -- a proxy layer that starts with compression but expands into observability, security, routing, and traffic management for all AI API calls. The Context Gateway architecture, written in Go and sitting between agents and LLM APIs, is the right abstraction for this. If compression is the wedge and the proxy becomes the platform, there's a real product-to-distribution trajectory. But I don't see evidence the founders are thinking this way yet. The 102 GitHub stars on Context Gateway suggest early tinkering, not a community forming around a new infrastructure primitive. And the competitive dynamics are brutal: Microsoft's LLMLingua is open-source with 5.8k stars and already integrated into LangChain and LlamaIndex. Any team that's comfortable self-hosting has a zero-cost alternative. Compresr must demonstrate materially superior compression quality to justify API fees, and no published benchmarks substantiate that claim.

The founding team has unusually strong domain fit for the specific problem -- Zakazov researched LLM context compression at EPFL, Gabouj worked on prompt compression at DLab -- but everyone on the team comes from academic or research backgrounds with intern-level industry experience. No prior startup exits, no evidence of commercial sales execution. They've shipped a product (SDK on PyPI, open-source proxy on GitHub), which is positive for pre-seed, but 102 GitHub stars and no public revenue signal suggest they're still in early exploration rather than rapid iteration with paying users. The four-person EPFL cohort reduces co-founder conflict risk through shared institutional context, but I worry about the transition from research to commercial execution. Selling infrastructure to enterprise engineering teams requires a different muscle than publishing at MICCAI.

The deeper problem from my framework: this business requires multiple things to go right simultaneously. The compression quality must materially exceed open-source alternatives. Token costs must remain a meaningful enterprise line item despite aggressive deflation. LLM providers must not ship native features that obviate the need for third-party compression. And the team must build enterprise sales capability from an academic starting point. That's not one miracle -- it's three or four sequential challenges, each with meaningfully uncertain outcomes. Compare this to Harvey, where the single miracle (model fidelity for legal work) was being delivered by the foundation model labs themselves. Compresr's miracles are all things the team must deliver against external forces actively working to make their product unnecessary.

I'm passing. The market exists but it's consensus-adjacent, not non-obvious. The structural moment is ambiguous -- agentic AI growth creates demand for compression, but platform pricing deflation and native caching features simultaneously erode it. The team has genuine research expertise in the specific problem domain, which I respect, but the business model looks more like a temporary arbitrage than a durable infrastructure layer. If token costs stabilize rather than continuing to fall, and if the Context Gateway becomes a genuine platform play beyond compression, this could work. But I'd need to see evidence of both before writing a check.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Non-Obvious Market at Structural Inflection | 10/35 |
| Product-to-Distribution Trajectory | 9/25 |
| Single-Miracle Operational Clarity | 5/15 |
| Founder Execution Velocity | 6/15 |
| Technology Cycle Positioning | 4/10 |
| **Total** | **34/100** |

**Total Score: 34/100** (Pass)
