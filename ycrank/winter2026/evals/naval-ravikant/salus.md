# Salus -- Naval Ravikant Evaluation

The first thing I notice about Salus is that OpenAI's own Agents SDK already has a guardrails section in its documentation. When the platform provider is building your product into their SDK, you don't have a company -- you have a feature request that someone else is going to ship for free. NVIDIA NeMo Guardrails is open-source with 5.6k stars. Galileo has $68M in funding and already offers real-time guardrails. The core concept here -- intercept a tool call, check it against a policy, block it if it fails -- is architecturally straightforward. This isn't thermodynamic computing or a novel database architecture. It's a conditional check inserted between an LLM's intent and its execution. Any competent team with SDK access to the agent framework can build this. The question isn't whether pre-execution validation is valuable -- it clearly is. The question is whether it's a company or a feature. Right now, the evidence points toward feature.

On specific knowledge: Vedant Singh has genuine research chops -- multiple Stanford labs, multi-agent systems work, the CryptoAgents project. That's more than most undergrad founders bring. But research experience with LLMs in academic settings is not the same as the deep, non-transferable knowledge of how agents fail in production. The insight that drives Salus -- "agents execute bad tool calls and you find out three hours later from a support ticket" -- is something anyone deploying agents for two months already knows. It's the complaint, not the cure, that requires specific knowledge. The founders who should be building this are the ones who've spent two years operating agent systems in production at scale, who've catalogued hundreds of failure modes, who can tell you the twelve ways a refund agent breaks that no one writes about in papers. That knowledge can't be taught in Stanford's labs. Singh's academic background is the teachable kind -- valuable, but replicable by any strong ML graduate.

The leverage architecture is real but limited. Software infrastructure that validates API calls is a fundamentally scalable model -- one API serving thousands of customers, usage-based pricing, no marginal human labor per validation. That's baseline software leverage. But compare this to what I look for in infrastructure plays: Stack Overflow created a knowledge graph that grew more valuable with every question. Alchemy built a developer platform that became the default abstraction layer for an entire protocol ecosystem. Salus inserts a check into someone else's workflow. The leverage is defensive -- preventing losses rather than enabling new capabilities. It doesn't create a new category of what's possible; it reduces the downside of an existing category. That's more like insurance than leverage, and insurance businesses compete on price, not on compounding technology.

The timing could not be less contrarian. "AI agents need guardrails" is not a secret -- it's a Gartner category. They've literally named it "guardian agents" and predicted it will capture 15% of the AI market. When Gartner has already taxonomized your market, you're not early and you're not non-consensus. You're riding a wave that every enterprise software vendor in the world is already paddling toward. Galileo, Fiddler, Patronus, Wayfound, and NeMo are all in some version of this space, with a combined $200M+ in funding. My best returns -- Twitter, Uber, Bitcoin -- came from bets where smart people said I was wrong. Nobody is going to tell the Salus founders they're wrong that agents need safety checks. That means the pricing already reflects the opportunity, and the competitive pressure is only going to intensify.

The bull case deserves honest examination. If agents become the primary interface between AI and the real world -- and that trajectory looks increasingly likely -- then the "firewall for AI agents" could become as essential as the network firewall was for internet infrastructure. Check Point and Palo Alto Networks built multi-billion-dollar businesses even though every router vendor offered basic packet filtering. The argument would be: platform-native guardrails will always be shallow and generic, while Salus builds deep, cross-platform validation that works across OpenAI, Anthropic, LangChain, and CrewAI simultaneously. The 58% blocked-action recovery rate suggests something genuinely useful is already working. And if they accumulate production failure-mode data across hundreds of deployments, that dataset becomes a real moat. For this bull case to materialize, two things must be true: the validation problem must be deep enough that platform-native solutions remain inadequate, and Salus must move fast enough to establish itself as the cross-platform standard before any single framework dominates. I'm not convinced either condition will hold.

Two Stanford undergrads with no enterprise sales experience, no public GitHub repos for their product, no documented revenue, and no company social media presence are entering a market where Galileo has six Fortune 50 customers and 834% revenue growth. The founders seem smart and capable, but intelligence and energy without specific knowledge and leverage architecture is just hard work in a competitive market. I need founders who know something the market doesn't. Here, the market already knows everything these founders know -- and it has a two-year head start.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Specific Knowledge and Founder-Problem Authenticity | 10/30 |
| Leverage Architecture and Scalability of the Model | 14/25 |
| Contrarian Positioning and Non-Consensus Timing | 5/20 |
| Founder Integrity and Long-Term Orientation | 9/15 |
| Technical Compounding and Defensibility Over Time | 4/10 |
| **Total** | **42/100** |

**Total Score: 42/100** (Pass)
