# Confluence Labs -- Paul Graham Evaluation

The first thing I notice about Confluence Labs is the gap between what they've proven and what they're selling. They scored 97.9% on ARC-AGI-2, a benchmark where the best commercial systems managed 37.6%. That's a genuinely impressive technical result from a two-person team. But ARC-AGI-2 tests whether AI can solve abstract grid puzzles from a few examples. The company's pitch is that this capability translates to designing pharmaceutical experiments, materials science workflows, and physics research. Those are completely different problems. Solving novel pattern-matching puzzles on synthetic grids is to designing a drug trial roughly what winning at chess is to running a war. The cognitive architecture might share some deep structure, but the surface-level skills, domain knowledge, and practical constraints are worlds apart. When the primary evidence for your company is a benchmark score, and the product requires something fundamentally different from what the benchmark measures, you have a proof-of-concept for a capability, not for a business.

The question I always ask first is: how did the founders find this problem? Did they stumble into it through their own work, or did they go looking for an application for their skills? Everything in this dossier points toward the second. Neither founder has a background in drug design, materials engineering, or the scientific domains they're targeting. Baskaran has genuine research credentials — RSI at 16, ISEF, behavioral psychology work at Vassar — which shows intellectual horsepower and some exposure to experimental methodology. But there's no story of a specific moment where either founder was personally stuck trying to design experiments in a data-sparse domain and thought "there has to be a better way." Instead, it reads like they built a very good AI reasoning system, noticed it was sample-efficient, and then cast around for industries where sample efficiency matters. That's the sequence I described in "Why Smart People Have Bad Ideas" — starting from your capabilities and searching for problems, rather than starting from a problem you can't stop thinking about. It's not a fatal flaw, but it means the founders don't have the bone-deep understanding of their customer's daily pain that makes early product decisions instinctively correct.

The schlep in their target markets is enormous, and I see no evidence they've reckoned with it. Selling AI tools to pharmaceutical R&D teams means navigating regulatory validation requirements, data governance policies, institutional review processes, and sales cycles that stretch across quarters. Materials science labs at companies like BASF have deeply entrenched workflows and internal tooling. These aren't customers you win with a GitHub repo and a benchmark score. The website says they're "looking for collaborators" — which is a polite way of saying they haven't found anyone willing to pay yet. Citrine Informatics has been building domain-specific credibility in materials science for twelve years. Recursion runs millions of biological experiments weekly with their own lab infrastructure. These incumbents didn't just build software; they embedded themselves in the operational reality of their customers' work. Confluence Labs hasn't begun that journey.

The strongest bull case is that these are genuinely talented young technical builders — Burdick taught himself to code, dropped out of college, and shipped products for six-plus startups; Baskaran was selected for RSI, one of roughly eighty students globally — and they achieved a state-of-the-art result that larger, better-funded teams couldn't match. If frontier LLM costs continue dropping and sample-efficient reasoning proves transferable to real-world experiment design, a small team with deep technical understanding of the approach could move faster than domain-specific competitors locked into narrow verticals. The domain-agnostic positioning is potentially powerful: rather than competing head-to-head with Citrine in materials or Recursion in biology, they could become the horizontal layer for experiment design across all data-sparse fields. Young founders have surprised me before — Steve Huffman and Alexis Ohanian were undergrads when they built Reddit in the first YC batch. But Reddit had users from day one. The distance between "we solve abstract puzzles efficiently" and "pharmaceutical companies depend on our experiment design" requires crossing a desert that's littered with the bones of AI research labs that never found product-market fit.

Two additional concerns compound the picture. First, they open-sourced their solver under MIT license. I understand the credibility play — it's how you establish reputation when you're unknown. But it means Poetiq, which just raised $45.8 million from former DeepMind researchers, can study every detail of their methodology and apply it with ten times the engineering resources. The open-source approach only works strategically if you're building proprietary value on top of it — domain-specific data, customer relationships, specialized models — and none of that exists yet. Second, the name. Naming your company "Confluence" when Atlassian's Confluence is one of the most widely-used enterprise software products on the planet is the kind of oversight that suggests the founders haven't thought carefully about distribution and discovery. It's a small thing, but relentlessly resourceful founders tend to notice these practical obstacles before they become problems.

I want to be clear: the technical talent here is real. A two-person team achieving a near-perfect score on a challenging AI benchmark while spending roughly twelve dollars per task is impressive engineering, not luck. But a startup isn't a research lab, and a benchmark isn't a product. The path from here to a business requires domain expertise they don't have, enterprise sales skills they haven't demonstrated, and a product that doesn't yet exist for customers they haven't identified. The idea was constructed from capability, not discovered from pain. That's a pattern I've seen fail many times — brilliant people building impressive technology that never finds its way to someone who needs it badly enough to pay.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 7/30 |
| Relentlessly Resourceful Founders | 13/25 |
| Evidence of Wanting: Demonstrated User Pull | 4/20 |
| Technical Hacker Founders Who Build | 12/15 |
| Growth Trajectory and Default Alive Economics | 4/10 |
| **Total** | **40/100** |

**Total Score: 40/100** (Pass)
