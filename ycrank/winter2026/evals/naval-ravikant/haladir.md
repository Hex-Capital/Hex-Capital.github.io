# Haladir -- Naval Ravikant Evaluation

The first thing I notice about Haladir isn't the idea -- it's the gap between the buyer and the seller. The customers for RL training infrastructure are Anthropic, OpenAI, and DeepMind -- organizations employing the planet's best researchers in reinforcement learning, operations research, and formal methods. The sellers are four undergraduate students with no documented industry experience, no published research, and no prior work in the domains they claim to serve. This isn't inherently disqualifying -- I've backed young founders before -- but my framework demands a specific question: what do these founders know about the intersection of classical solvers and RL training that the research scientists at their target customers don't already know? The dossier provides no answer. An OR-bench repository covering ten optimization problem types demonstrates competence with OR-Tools documentation, not the kind of deep, non-transferable understanding that took Amjad Masad years of building coding tools to develop before Replit existed.

The structural thesis, divorced from the team, has genuine appeal. Classical solvers as oracle machines for RL rewards -- that's an infrastructure play I'd normally lean into. It's picks-and-shovels logic applied to the RL training boom: build the verified ground-truth pipeline once, sell it to every lab training reasoning models. The leverage mechanics are sound. One well-designed data generation system could serve the entire frontier model ecosystem. And the domain positioning is legitimately differentiated -- while Applied Compute and Mechanize cluster around coding tasks, operations research and formal verification represent orthogonal sources of verifiable reward signal. If someone with twenty years of OR expertise and deep AI lab relationships were building this, I'd be writing a check.

But specific knowledge is my first filter, not my last. The product focus ambiguity compounds my concern. The GitHub organization describes "AI-Enabled Mainframe Modernization and Code Translation." The YC description pitches RL infrastructure. Rosetta is a COBOL modernization tool. OR-bench is an optimization benchmark. These could be connected -- COBOL translation as formally verifiable RL training data -- but the spread looks more like founders searching for product-market fit than executing from deep domain conviction. When I backed Notion, Ivan Zhao wasn't simultaneously exploring three different product categories. He was obsessively building one tool because he understood something specific about how individuals organize thought. Haladir's bifurcated approach signals that the founders haven't yet identified their irreplaceable insight.

The bull case is real and worth stating precisely. If verifiable-domain RL training data becomes the bottleneck for reasoning model improvement -- and Anthropic's reported $1B+ budget suggests it might -- then the first team to establish quality benchmarks in operations research could own a meaningful niche. OR problems have a beautiful property for RL: solvers compute provably optimal solutions, creating unambiguous reward signals. A young team with the audacity to move fast could establish themselves before Applied Compute or Scale AI expand beyond coding. The timing window exists. The question is whether audacity plus competence equals execution against the world's most sophisticated buyers -- and without demonstrated specific knowledge in the domain, I think the answer is probably no. A smart team at Anthropic or Applied Compute could replicate the OR-bench approach in weeks, not months.

The contrarian angle is partial at best. Yes, OR-domain RL training data is underserved. But the broader category -- RL environment infrastructure -- is one of the hottest markets in AI right now. Applied Compute hit a $1.3B valuation. Mechanize is paying engineers $500K. This isn't the kind of contrarian timing that made Uber at $4M or Bitcoin in 2014 exceptional bets. It's a differentiated niche within a consensus wave, which limits the asymmetric upside I optimize for. The pricing already reflects optimism about RL environments as a category; the only contrarian element is the specific domain choice, and that's insufficiently contrarian to generate the non-consensus returns I seek.

I'm passing. The architecture is interesting but the specific knowledge isn't there. Four undergraduates with no demonstrated depth in operations research, formal verification, or RL systems are attempting to sell infrastructure to the most technically sophisticated organizations in the world. The product focus split between COBOL modernization and RL training data suggests they're still discovering what they know, not deploying knowledge they already have. I want founders where the specific knowledge precedes the company -- where the company is the inevitable expression of something they've been learning for years. Haladir feels like the company preceded the knowledge.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Specific Knowledge and Founder-Problem Authenticity | 6/30 |
| Leverage Architecture and Scalability of the Model | 15/25 |
| Contrarian Positioning and Non-Consensus Timing | 11/20 |
| Founder Integrity and Long-Term Orientation | 7/15 |
| Technical Compounding and Defensibility Over Time | 5/10 |
| **Total** | **44/100** |

**Total Score: 44/100** (Pass)
