# Haladir -- Sam Lessin Evaluation

Here's the structural problem with Haladir before I even get to the team: this is an AI-native company selling infrastructure to AI labs, built entirely on open-source solvers. Strip away the AI training paradigm and you have OR-Tools and TLA+ provers -- tools that already exist, that anyone can download, that have been around for decades. There is no business without the current RL-with-verifiable-rewards moment. That's the exact inverse of what I look for. My "cherry on top" test asks whether a business would be good if AI didn't exist. Here the answer isn't just "no" -- it's "there would literally be nothing." The company IS an AI infrastructure play, which means I'm being asked to bet that the specific RL training paradigm holds steady, that AI labs continue outsourcing environment construction rather than building in-house, and that four undergraduates can out-execute Applied Compute ($100M+ raised, ex-OpenAI founders) and Scale AI ($29B valuation with a dedicated RL environments team). That's a lot of fragile assumptions stacked on top of each other.

The sane-person-insane-idea calibration is off in both directions. The founding team -- a CMU freshman, a Princeton CS student, and two UVA system undergrads -- doesn't yet have the "sane" credentials to anchor a bet in this space. When you're selling to Anthropic's RL team, your counterparts have PhDs in machine learning and decades of combined research experience. I backed Solana because Anatoly Yakovenko brought Qualcomm systems engineering credibility to a genuinely insane idea (yet another Layer 1 during crypto winter). The insane idea was the point, and the credibility made it investable. With Haladir, the idea -- using classical solver-computed ground truth as RL rewards -- is actually quite logical. It's a smart observation that operations research problems have verifiable solutions and are underserved as training domains. But "smart observation" is not "insane idea." Any sharp ML researcher at a frontier lab could arrive at the same insight, and several probably already have. So we get the worst possible combination in my framework: unproven founders with a reasonable idea. The non-consensus premium has been arbitraged away before there's even a market.

The moat question is where this falls apart entirely. The dossier itself acknowledges it: "The underlying solver technology (OR-Tools, TLA+ provers) is open-source and accessible. Other teams could build similar data pipelines." The claimed barrier is interdisciplinary expertise -- knowing how to bridge OR solvers with RL training pipelines simultaneously. But expertise is the most ephemeral moat in technology. When Anthropic is reportedly willing to spend $1B+ on RL environments and paying engineers $500K at Mechanize to build them, the "we understand both domains" advantage disappears the moment a well-funded competitor hires two OR PhD students. There's no physical infrastructure here, no proprietary data that compounds through usage, no protocol-layer lock-in. This is knowledge packaged as software -- precisely the category I've been arguing gets destroyed as AI makes development cheaper and faster.

Let me engage genuinely with the bull case, because there is one. The timing could be extraordinary: RLVR is the dominant paradigm right now, labs are spending aggressively, and OR/formal-verification domains are a genuine gap in the training data landscape. If Haladir moves fast enough, the OR-bench repository could become a de facto standard for evaluating model reasoning on optimization problems -- and standards, once established, have protocol-like stickiness. The COBOL modernization angle (Rosetta) is actually clever as a dual-use play: enterprise revenue from legacy code migration generates formally verifiable translation pairs that feed the RL training pipeline. Two businesses creating a data flywheel. And YC plus Susa Ventures provides real credibility scaffolding for a young team. If the RL training market grows at the projected 25.8% CAGR and Haladir carves out the verifiable-domains niche before anyone else, the ceiling is genuinely high. But "if they move fast enough, before anyone else, and the paradigm holds" is three conditional clauses too many for my money. The product focus ambiguity between COBOL modernization and RL training data -- their GitHub says "AI-Enabled Mainframe Modernization" while YC says RL infrastructure -- suggests the team hasn't yet resolved which business they're building, which is exactly the wrong signal when the competitive window is this narrow.

I keep coming back to the competitive dynamics. Applied Compute has $100M+ and is reportedly in talks at a $1.3B valuation with ex-OpenAI founders and customers like DoorDash and Cognition AI. Mechanize has a direct Anthropic partnership. Scale AI could expand into this domain with a single product decision. The Wing VC analysis predicts consolidation to 3-5 winners by 2030. In a market that rewards research credibility, lab relationships, and capital-intensive environment construction, four undergraduates with a GitHub org showing 3 followers and 5 total stars face existential headwinds. I'd rather fund a company where operational complexity creates the moat than one where the moat depends on staying ahead of better-funded teams in a domain where the underlying tools are freely available.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| "Sane Person, Insane Idea" Calibration | 8/30 |
| AI-Resistant Structural Moat | 5/25 |
| New Primitive or Protocol Position | 7/20 |
| Narrative Magnitude and Infinity Optionality | 8/15 |
| Real Revenue or Asset Cushion | 2/10 |
| **Total** | **30/100** |

**Total Score: 30/100** (Pass)
