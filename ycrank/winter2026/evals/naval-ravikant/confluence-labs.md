# Confluence Labs -- Naval Ravikant Evaluation

The first thing that strikes me about Confluence Labs is the distance between the signal and the substance. A 97.9% score on ARC-AGI-2 -- where pure LLMs score zero and the best commercial systems managed 37.6% -- is a genuine technical achievement. That's not noise; that's evidence of people who understand something about structuring reasoning that most AI researchers don't. But then they open-sourced it under MIT license and put up a website that says "looking for collaborators." They've demonstrated capability without creating capture. In my framework, that's a research contribution, not a business. The benchmark is synthetic grid puzzles. The claimed customers are pharma R&D teams and materials science labs. The gap between those two things is not an engineering sprint -- it's years of domain immersion they haven't started.

The specific knowledge question is where this falls apart for me. Niranjan Baskaran is clearly talented -- RSI at 16, ISEF awards, Atlas Fellow. That's raw intellectual horsepower. Brent Burdick taught himself to code and shipped products for six startups. Both are impressive for their age. But what do they know about drug design that a medicinal chemist at Novartis doesn't? What do they understand about materials engineering that a researcher at BASF hasn't already internalized? The answer, based on everything in this dossier, is nothing. Their knowledge is in AI reasoning -- how to structure problems so LLMs can solve them with minimal examples. That's a real skill. But it's a general-purpose AI capability, and in 2026, every frontier lab, every well-funded startup, every PhD student at Berkeley is working on reasoning efficiency. When I backed Amjad Masad at Replit, he'd spent years building coding tools -- the problem lived in his bones. Here, the founders are pointed at scientific experiment design from the outside, armed with a benchmark score and enthusiasm. That's the difference between specific knowledge and adjacent intelligence.

The leverage architecture concerns me equally. Enterprise R&D sales to pharma companies and materials science labs are among the least leveraged go-to-market motions in technology. Each customer requires domain customization, long sales cycles, regulatory awareness, and deep integration into existing workflows. Recursion Pharmaceuticals runs millions of biological experiments weekly with proprietary lab infrastructure. Citrine has twelve years of accumulated materials data and a BASF partnership. Confluence Labs has two founders in their early twenties with no domain relationships, no sales infrastructure, and no product. Even if the underlying technology is sound, the delivery mechanism is linear, not leveraged. Compare this to how Stack Overflow worked: build it once, developers come to you, every question and answer makes the graph more valuable. There's no equivalent flywheel here -- each new scientific domain requires starting the credibility engine from scratch.

The timing is wrong in a way that compounds the other problems. "AI for science" is the hottest consensus category in venture capital right now. Lila Sciences raised a $200M seed. CuspAI closed $100M. Periodic Labs took $300M. Poetiq -- which is solving essentially the same meta-problem of enhancing LLM reasoning -- raised $45.8M from former DeepMind researchers. When the market is this flush, pricing already reflects optimism, and the competitive landscape punishes underfunded teams. Two founders with YC funding are entering a domain where competitors have hundred-million-dollar war chests and domain-specific data moats. The contrarian angle -- that sample efficiency matters more than data scale -- has some intellectual merit, but it's not contrarian enough when every major lab is also pursuing reasoning improvements.

The bull case is worth stating clearly: if data-efficient reasoning is the fundamental bottleneck in AI for science, and if these founders have cracked something about how to achieve it that the 97.9% score reflects, then being domain-agnostic is the right positioning -- they'd be the infrastructure layer, not the application. The ARC-AGI-2 result could be to experiment design what PageRank was to search: a general capability that transforms every domain it touches. Baskaran's trajectory suggests someone drawn to deep problems rather than trend-surfing. And at pre-seed, raw technical talent occasionally matters more than domain expertise because the domain knowledge can be acquired through early partnerships. If they found one pharma collaborator willing to run real experiments using their framework, and the results were measurably better than traditional design-of-experiments methods, the entire thesis changes. That's a real scenario -- I just don't see enough evidence to bet my own money on it materializing against competitors with fifty to three hundred times more capital.

What I'd need to invest: evidence that their reasoning approach translates to a real scientific domain -- one pilot, one dataset, one experiment that worked better than the baseline method in an actual lab. Without that, this is a research lab that has demonstrated intelligence and energy but hasn't yet shown the specific knowledge or leverage architecture that turns research into a compounding business. I'm passing.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Specific Knowledge and Founder-Problem Authenticity | 10/30 |
| Leverage Architecture and Scalability of the Model | 10/25 |
| Contrarian Positioning and Non-Consensus Timing | 9/20 |
| Founder Integrity and Long-Term Orientation | 8/15 |
| Technical Compounding and Defensibility Over Time | 4/10 |
| **Total** | **41/100** |

**Total Score: 41/100** (Pass)
