# Compresr -- Sam Lessin Evaluation

Here's the structural problem with Compresr that I can't get past: this is a company whose entire reason to exist is that LLM tokens are expensive, and it's launching into a market where token costs have been falling roughly 10x per year. That's not a tailwind -- that's building your house on a melting glacier. Every pricing cut from OpenAI, every context window expansion from Anthropic, every efficiency gain at the inference layer chips away at the urgency of what Compresr sells. The total dollar spend on inference is growing, yes -- it doubled to $8.4B -- but that growth comes from usage expansion, not from developers desperately seeking ways to compress their prompts. When I ask my core question -- does this business get stronger or weaker as software gets cheaper? -- the answer here is unambiguous. It gets weaker. That's the opposite of where I want to be.

The founder-idea pairing fails my primary filter, but not in the way most companies fail it. The founders are genuinely credible in the specific domain -- Ivan Zakazov researched LLM context compression at EPFL's DLab, Oussama Gabouj worked on prompt compression and efficient ML systems, they have real publications. These are sane people. But the idea is also sane. Everyone building production LLM applications knows token costs matter. Microsoft Research already built the open-source version -- LLMLingua has 5.8k GitHub stars and is integrated into LangChain and LlamaIndex. When I backed Venmo, nobody thought peer-to-peer mobile payments were necessary. When Slow seeded Solana, most VCs had written off new Layer 1 blockchains entirely. Those were sane-person-insane-idea pairings. "Let's build a managed API for compressing LLM prompts" is a sane person with an entirely sensible, consensus-validated idea -- and that pairing produces the returns that are already priced into the market.

The moat question is where this really falls apart for me. Compresr's defensibility is entirely software-based: proprietary compression algorithms that "outperform traditional retrieval baselines" -- though no published benchmarks exist to verify this claim. Microsoft's LLMLingua is free and achieves 20x compression. Any team with ML expertise and access to the published academic literature (EMNLP'23, ACL'24, NAACL'25) can build a competitive implementation. And the platforms themselves are already moving: Anthropic shipped prompt caching, OpenAI offers cached context pricing discounts. When I think about Craftwork -- where the moat is painters on ladders, scheduling logistics, quality control in physical homes -- the contrast could not be sharper. You can't open-source a painting crew. You absolutely can open-source a compression algorithm.

Let me steelman the bull case, because there is one worth articulating. The argument goes: Compresr becomes the Cloudflare of LLM inference -- a middleware proxy that every production AI application routes through. The Context Gateway architecture, sitting between agents and model APIs, could create switching costs and data flywheel effects. LLM providers face a structural disincentive to offer compression because it cannibalizes their per-token revenue, creating a durable window for a third party. And the total addressable problem is growing -- agentic workflows with multi-step tool traces and massive conversation histories consume tokens at rates that make compression economically compelling regardless of per-token price declines. If all of that were true, you'd have something approaching a new primitive in the inference stack. But I'd need to see evidence that the compression quality is materially better than LLMLingua, that developers are choosing to route production traffic through a third-party proxy rather than self-hosting, and that the switching costs are real. A hundred GitHub stars doesn't tell me any of that. The theoretical protocol position exists on the whiteboard but not in the market.

The narrative ceiling is what ultimately caps this for me. An "infinity story" needs to be able to absorb new developments and grow. Compresr's narrative shrinks with every new development in the AI ecosystem -- cheaper models, longer context windows, native caching, better retrieval. The company is swimming against the current of the most powerful deflationary force in technology. Even the "why now" framing in the dossier acknowledges this tension: costs are declining 10x annually but total spend is growing. That's a description of a transitional moment, not a durable structural opportunity. The Birchbox lesson I carry -- narrative magnitude without structural moat produces the worst venture outcome -- applies directly here, except the narrative itself has a visible expiration date. This is a feature waiting to be absorbed, not a company waiting to compound.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| "Sane Person, Insane Idea" Calibration | 8/30 |
| AI-Resistant Structural Moat | 4/25 |
| New Primitive or Protocol Position | 6/20 |
| Narrative Magnitude and Infinity Optionality | 5/15 |
| Real Revenue or Asset Cushion | 2/10 |
| **Total** | **25/100** |

**Total Score: 25/100** (Strong Pass)
