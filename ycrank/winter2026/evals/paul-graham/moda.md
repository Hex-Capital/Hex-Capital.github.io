# Moda -- Paul Graham Evaluation

The first thing I notice about Moda is the landscape. "Sentry for AI" is a sharp tagline, but AI observability is one of the most crowded categories in the current batch environment. Braintrust just raised $80M at an $800M valuation. Arize has $131M and Datadog as a strategic investor. Langfuse was acquired by ClickHouse. Helicone raised $5M. AgentOps raised a pre-seed round. That's not a category hiding in plain sight because of schlep blindness -- that's a category lit up in neon with hundreds of millions of dollars pouring into it. When I wrote about schlep blindness with Stripe, the critical fact was that thousands of programmers knew payments were broken and nobody had tried to fix it properly because the banking and regulatory complexity was so ugly. The AI monitoring problem is the opposite: everybody sees it and everybody is already working on it. That's not the kind of market where a two-person team starts with a structural advantage.

The organic discovery question is more interesting than the market would suggest. Pranav Bedi spent time at Cerebral Valley building LLM observability -- token tracking, cost monitoring, agentic deep web scraping. He was inside the machine and presumably saw firsthand that tracking tokens and latency misses the behavioral failures that actually matter. That's a real lived-experience signal. It's not as strong as Drew Houston forgetting his USB drive -- the discovery feels more like professional observation than personal frustration -- but it's not manufactured from a brainstorming session either. The complication is that the founders applied to YC with a different idea first, pivoted after it failed to gain traction, and landed on AI agent monitoring. Pivots happen -- Justin.tv became Twitch, and that pivot was one of the best outcomes in YC history. But the pivot trajectory here suggests the founders chose this problem partly because the market is hot, not purely because they were compelled by the specific failure mode.

Mohammed Al-Rasheed's backstory is the strongest founder signal. He built HookedIn.ca -- a real-time crash alert platform for tow truck drivers -- and scaled it to $125K monthly recurring revenue across 28 cities with 6,000 users before Ontario regulation shut down tow truck operations. That's not a line on a resume; that's a founder who built a real business, found users, scaled it, and got killed by forces outside his control. The resilience to come back and start again is exactly the kind of thing I look for. And the co-founder relationship is solid: Waterloo classmates who both worked at Shopify. That's not two strangers who met at a hackathon -- they have overlapping history across education and employment. I've seen too many teams disintegrate when the co-founders barely know each other. This pair doesn't trigger that concern.

The bull case would require Moda to be Sentry, not another APM dashboard. Sentry carved out a billion-dollar niche by being the best at one thing -- error monitoring -- while Datadog and New Relic were broad observability platforms. If Moda can own behavioral failure detection the way Sentry owns crash reporting, the two-line SDK integration and narrow focus become advantages rather than limitations. Enterprise-priced incumbents like Arize ($50K-$100K/year) leave a massive gap for developer-first tooling. The "plain-language signal creation" feature is clever -- letting teams encode institutional knowledge about what constitutes a failure in their specific context. If teams build libraries of custom signals, switching costs develop naturally. For this to work, Moda needs the kind of ferocious bottom-up adoption that Sentry achieved: individual developers installing it because it makes their lives measurably better, not because procurement approved a vendor. The founders are technical and can build -- both are ex-Shopify engineers, both have shipped products. The capability is there. What I lack is evidence that the product is already making someone's life better.

My core concern is the absence of any pull signal combined with the intensity of competition. Private beta, no public users, no revenue, no waitlist data, no organic sharing. At pre-seed I don't demand much -- a handful of teams who installed the SDK and sent excited messages would be enough. A single Slack screenshot of someone saying "this caught a failure we never would have found" would shift my assessment materially. Without that, I'm looking at a competent team with relevant experience entering one of the most well-funded competitive fields in software. The technical problem -- scanning conversations for semantic failures -- is reproducible by any team with access to the same LLMs. The plain-language signal creation feature uses capabilities available to everyone. The narrow positioning is smart but not defensible by itself. I'd want to see this team prove that developers love the product before I'd write a check from my own money, because in a market this crowded, user love is the only thing that matters. Everything else is noise.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 14/30 |
| Relentlessly Resourceful Founders | 15/25 |
| Evidence of Wanting: Demonstrated User Pull | 5/20 |
| Technical Hacker Founders Who Build | 11/15 |
| Growth Trajectory and Default Alive Economics | 5/10 |
| **Total** | **50/100** |

**Total Score: 50/100** (Neutral)
