# ARC Prize Foundation -- Paul Graham Evaluation

The most striking thing about ARC Prize is that François Chollet didn't find this problem -- he *named* it. In 2019, while at Google, he published "On the Measure of Intelligence," which argued that the entire field was measuring the wrong thing: memorized skill rather than the ability to acquire new skills efficiently. Then he built ARC as the alternative. That's a level of organic problem discovery I almost never see. Most founders stumble into a problem through lived experience. Chollet went further -- he lived inside AI research for a decade, identified a foundational measurement failure that the whole field was unconsciously working around, and then formalized both the diagnosis and the cure. The schlep here is real but unusual: it's not regulatory paperwork or bank integrations. It's the intellectual labor of designing tasks that resist brute-force memorization while remaining solvable by any human in two attempts, then calibrating them with 400+ human participants, then convincing an entire industry to voluntarily adopt your yardstick. That combination of intellectual depth and political coordination is exactly the kind of work most smart people avoid because there's no obvious payoff.

The team composition here is unusual and strong in the right ways. Mike Knoop co-founded Zapier in YC S12, which now generates $310M in annual revenue at a $5B valuation. He doesn't need to do this. He shifted from running Zapier's product org to leading their AI R&D, then left to work on AGI measurement. That trajectory tells me something about earnestness -- he moved toward the harder, more fundamental problem rather than staying where the money was comfortable. Greg Kamradt independently created the "Needle in a Haystack" context retrieval test that became a de facto standard for evaluating LLM context windows. So two of the three founders have already created evaluation tools that the AI community adopted organically. Chollet created Keras, which has 63.8K GitHub stars and over 2 million users. These are builders. They've already built the thing. The benchmark exists, the competition infrastructure works, the academic panel is assembled.

The user pull signal here is the strongest I can identify at pre-seed. Four frontier labs -- OpenAI, Anthropic, Google DeepMind, and xAI -- voluntarily report ARC-AGI scores in their model cards. Nobody made them do this. When OpenAI launched o3 in December 2024, they chose to benchmark it against ARC-AGI publicly. That's the equivalent of early Airbnb hosts who "actually needed to do these rentals to pay their rents" -- except here it's the biggest AI labs in the world using ARC-AGI because they need a credible reasoning benchmark and nothing else fills the gap. Add 1,455 competition teams, 90 research papers, and 4,437 Discord members, and you have genuine community pull that wasn't manufactured through marketing spend.

Now, the tension I have to address directly: this is a 501(c)(3) nonprofit. There is no equity, no revenue model, no path to a financial return on an angel check. My framework for evaluating startups -- "default alive," ramen profitability, week-over-week growth -- breaks down when applied to a donation-funded research institution. The donors are impressive (Andy Fang from DoorDash, Dharmesh Shah from HubSpot, Aaron Levie from Box), but they're making philanthropic contributions, not investments. I can evaluate whether this is important work done by exceptional people -- and it clearly is -- but the economic structure means my criteria for growth trajectory and sustainability don't apply in their normal form. The organization depends on continuous fundraising from the very labs it evaluates, which creates a structural tension that would concern me even in a for-profit context.

The bull case is genuinely compelling: if AI regulation matures (EU AI Act, U.S. AI Safety Institute), governments will need independent evaluation infrastructure they can trust. ARC Prize is better positioned than anyone to be that standard -- more credible than Scale AI (which has commercial relationships with the labs it evaluates), more focused than Epoch AI (which aggregates rather than creates), and more research-oriented than METR (which focuses on safety rather than capability measurement). In a world where AI evaluation becomes as important as financial auditing, ARC Prize could become the GAAP of artificial intelligence. The team has the intellectual authority, the community adoption, and the institutional independence to fill that role. If I were making a philanthropic bet on AI infrastructure rather than an angel investment seeking returns, this would be one of the most compelling opportunities I've seen.

What holds me back from scoring higher is the combination of nonprofit economics and the benchmark treadmill problem. ARC-AGI-1 went from 33% SOTA to OpenAI's o3 scoring 87.5% within a single year. The Foundation must continuously design harder benchmarks faster than AI capabilities advance -- and each new version (like ARC-AGI-3's shift to interactive environments) risks fragmenting the community that adopted the previous format. This is a Red Queen's race where you have to keep running to stay in place, funded entirely by donations. That's a precarious position even for the most capable team.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 25/30 |
| Relentlessly Resourceful Founders | 17/25 |
| Evidence of Wanting: Demonstrated User Pull | 16/20 |
| Technical Hacker Founders Who Build | 13/15 |
| Growth Trajectory and Default Alive Economics | 3/10 |
| **Total** | **74/100** |

**Total Score: 74/100** (Invest)
