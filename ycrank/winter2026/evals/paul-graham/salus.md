# Salus -- Paul Graham Evaluation

The first thing I notice about Salus is what's missing from the problem: the schlep. When I think about the companies that worked best from YC, the common thread isn't just that they solved a real problem -- it's that the problem involved painful, tedious complexity that made everyone else unconsciously look away. Stripe had to deal with banks, fraud systems, and financial regulation. Airbnb had to build trust between strangers sleeping in each other's homes. Those problems were hiding in plain sight because the work required to solve them was so ugly. With Salus, I'm looking at a problem -- validating agent tool calls before they execute -- that the dossier itself describes as "architecturally straightforward." Any observability vendor with SDK access could build this. OpenAI already has a guardrails section in their Agents SDK documentation. When the core technical challenge can be described as intercepting a function call and checking it against a policy file, there's no schlep wall keeping competitors out. That's the opposite of what I want to see.

The origin story also concerns me. Two Stanford CS roommates building an AI safety product during the biggest AI hype cycle in history. Vedant Singh has genuine research depth -- multi-agent systems, LLM reasoning, the CryptoAgents trading project -- so it's plausible he encountered agents misbehaving while building agentic systems. That's not nothing. But "plausible" isn't "organic." I don't see the equivalent of Drew Houston forgetting his USB drive -- the specific, personal moment where the problem became impossible to ignore. The product's marketing example ("Your agent processed a refund without looking up the order ID, costing you thousands") reads like a scenario constructed to sell the category, not a war story from production. When I wrote about how to get startup ideas, the key distinction was between ideas you stumble into through lived experience and ideas you generate by asking "what's a good startup idea in AI safety?" This pattern -- smart technical students scanning a hot market for problems to solve -- is exactly the failure mode I've spent years writing about.

The competitive landscape reinforces my concern. Galileo has raised $68M and reports 834% revenue growth. Fiddler has raised around $100M. Patronus has $40M. NVIDIA's NeMo Guardrails is open-source with 5,600 GitHub stars. Wayfound is building the "guardian agent" category with Gartner's backing. This is not a market hiding in plain sight -- it's a market where everyone with a TechCrunch subscription knows the opportunity exists. When I noted in 2025 that the most impressive YC companies were not working on AI, this is what I meant. Consensus categories attract capital and competition faster than startups can build moats. Salus is a two-person undergraduate team competing against companies with tens of millions in funding and enterprise sales teams already in the field.

Let me steel-man this, though. The bull case for Salus is genuinely interesting. The shift from LLMs-as-chatbots to LLMs-as-agents that execute real-world actions is a phase change, not a trend. It's the difference between a model generating bad text and a model sending your customer a refund it shouldn't have. The 58% blocked-action recovery rate -- where agents self-correct after receiving structured feedback about what went wrong -- suggests the team has built something with real engineering thought behind it, not just a policy-matching wrapper. If agent deployment explodes the way the market projections suggest, and if the developer-experience moat (single decorator, framework-agnostic) creates genuine switching costs, Salus could become the Stripe of agent safety -- the thing every developer reaches for because the integration is so clean nobody bothers building it themselves. The narrow focus on pre-execution blocking, versus the broader platforms trying to be everything, could be an advantage. For this to work, though, you need the framework vendors to not build it themselves, the enterprise competitors to remain too broad, and two undergraduates to out-execute teams with 100x their resources. That's at least two miracles, maybe three.

Vedant Singh's technical credentials are real. Research across Stanford's Open Virtual Assistant Lab, Legal Design Lab, and multiple other groups, plus the Linguistics Olympiad medal and MIT RSI -- this is someone who works hard and is genuinely smart. The CryptoAgents project shows he can build multi-agent systems, not just study them. But academic intensity and startup intensity are different animals. I haven't seen evidence of either founder facing a serious obstacle and inventing a creative way around it. No prior startup, no failure-and-comeback story, no signal of the specific conjunction of determination and adaptability that I look for. They're talented students building their first company. That's not disqualifying -- Reddit's founders were young too -- but it means I'm betting on potential rather than demonstrated resilience, in a market that demands enterprise credibility.

The absence of any public user signal is hard to overlook even at pre-seed. No user counts, no waitlist, no community, no GitHub organization, no Product Hunt launch. For a developer tool with an API-first model, the natural distribution channel is open-source adoption or at least public developer engagement. The benchmarks they cite (52% misalignment reduction, 60% lower cost) appear to be from internal testing rather than production deployments. I don't see evidence that anyone outside the team has integrated this into a real agent workflow. At this stage, even a handful of developers saying "I tried this and it caught a bug my agent would have shipped" would shift my assessment meaningfully.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Organic Problem Discovery and Schlep Willingness | 8/30 |
| Relentlessly Resourceful Founders | 10/25 |
| Evidence of Wanting: Demonstrated User Pull | 5/20 |
| Technical Hacker Founders Who Build | 9/15 |
| Growth Trajectory and Default Alive Economics | 5/10 |
| **Total** | **37/100** |

**Total Score: 37/100** (Pass)
