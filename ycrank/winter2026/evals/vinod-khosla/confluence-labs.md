# Confluence Labs -- Vinod Khosla Evaluation

The most striking thing about Confluence Labs is what I call the "benchmark-as-company" problem. They scored 97.9% on ARC-AGI-2 -- synthetic grid puzzles -- and from that achievement, they're building a thesis about transforming drug discovery, materials engineering, and physics research. I've been on the wrong side of this exact structure before. KiOR produced cellulosic biofuel in a lab. The technology worked. The economics didn't. The distance between a technical demonstration and a business that changes an industry is not a gap you bridge with optimism -- it's a chasm that has consumed billions of dollars of capital, including $160 million of my own money. Solving abstract pattern recognition tasks from a few examples is genuinely impressive. It is also structurally different from designing a maximally informative experiment in a pharmaceutical R&D pipeline. No evidence in this dossier suggests they've even attempted the translation.

Let me be clear about the consequence magnitude: if someone builds AI that reduces the number of physical experiments needed to discover a new drug or material by an order of magnitude, that changes everything. Drug development costs $2.6 billion per approved compound, and the failure rate is 90%. Materials engineering underpins trillions of dollars of physical economy. The problem is not small. But Confluence Labs hasn't articulated a specific transformational outcome -- they've described a capability ("data-efficient modeling") and gestured at multiple verticals simultaneously. "We're looking for collaborators in hardware engineering, biology, materials science, and physics" is the language of a research lab exploring possibilities, not a company with a thesis about which industry it intends to make obsolete. The problem space deserves ambition. The company hasn't yet matched its ambition to a specific domain where it will prove the approach works economically.

The founders show genuine intellectual horsepower. Baskaran was selected for RSI -- roughly 80 students globally -- and competed at ISEF, which signals raw cognitive ability at a high level. Burdick dropped out of college to teach himself engineering and has shipped products across six startups. The ARC-AGI-2 result itself is evidence that a two-person team can outperform well-funded research groups on a specific technical challenge. These are bright, capable people. But I don't see contrarian courage here. "AI for science" is not a contrarian bet in 2026. Lila Sciences raised $200 million, Periodic Labs $300 million, CuspAI $100 million, Poetiq $45.8 million -- all in the last year. When I backed OpenAI in 2019, AI was laughable and I was the only VC in the room. Confluence Labs is entering a field where hundreds of millions of dollars are already deployed by teams with domain-specific datasets, enterprise partnerships, and in some cases physical robotic lab infrastructure. Being smart is necessary but insufficient when Recursion is running millions of biological experiments per week and Citrine has twelve years of materials data.

The bull case is worth engaging seriously. If these founders are the kind of athletes who learn at exponential rates -- and the RSI selection, the benchmark achievement, and the self-taught engineering trajectory suggest they might be -- then domain expertise is acquirable. The best outsiders learn domains faster than insiders can unlearn their biases. If LLM capabilities continue their current trajectory, the core approach of using language models for program synthesis in data-sparse domains becomes more powerful every quarter without Confluence Labs having to do anything. And the open-sourcing is potentially a strength, not a weakness: it builds credibility in the research community, attracts collaborators, and signals that the team's value lies in their ability to translate the methodology to specific domains, not in the code itself. If they pick one domain -- say, small-molecule drug design -- and demonstrate that their approach reduces the experimental cycle by 5-10x for a real pharmaceutical client, the company becomes very interesting very quickly. The rate of change in underlying LLM capabilities is genuinely favorable, and inference costs dropping makes compute-intensive experiment design economically viable for the first time.

But the evidence today doesn't support that narrative yet. They have no product, no customers, no pilots, no domain partnerships, and no revenue. Their primary technical differentiator is open-sourced under MIT license. The benchmark that serves as their credibility signal tests abstract grid-puzzle reasoning, not scientific experiment design. Their competitors have 50-200x more capital. And critically, neither founder has deep expertise in any of the target verticals they've listed. Selling AI-driven experiment design to pharmaceutical R&D teams requires understanding not just the computation but the biology, the regulatory context, the clinical workflow, and the organizational politics of how experiments get approved and funded. This is the gene pool engineering problem: the team is built for benchmark performance, not for the domain translation that will determine whether this becomes a business. I'd want to see them pick a single domain, recruit someone who has lived inside that domain's experimental workflow, and demonstrate economic impact on a real problem -- not another benchmark. Until then, this is a science project with impressive credentials but no evidence it will produce a product that makes an existing system economically obsolete.

### Dimension Scores

| Criterion | Score |
|-----------|-------|
| Consequence Magnitude If Successful | 17/30 |
| Founder Learning Rate and Contrarian Courage | 14/25 |
| Technology Disruption Potential vs. Incumbent Systems | 8/20 |
| Rate of Change and Timing Trajectory | 9/15 |
| Gene Pool Engineering and Team Construction | 5/10 |
| **Total** | **53/100** |

**Total Score: 53/100** (Neutral)
